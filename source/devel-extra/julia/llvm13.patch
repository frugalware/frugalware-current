diff -Naur julia-1.7.3/src/abi_aarch64.cpp julia-1.7.3.new/src/abi_aarch64.cpp
--- julia-1.7.3/src/abi_aarch64.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/abi_aarch64.cpp	2022-07-16 20:42:45.791771286 +0200
@@ -13,7 +13,7 @@
 
 struct ABI_AArch64Layout : AbiLayout {
 
-Type *get_llvm_vectype(jl_datatype_t *dt) const
+Type *get_llvm_vectype(jl_datatype_t *dt, LLVMContext &ctx) const
 {
     // Assume jl_is_datatype(dt) && !jl_is_abstracttype(dt)
     // `!dt->name->mutabl && dt->pointerfree && !dt->haspadding && dt->nfields > 0`
@@ -23,18 +23,16 @@
     assert(nfields > 0);
     if (nfields < 2)
         return nullptr;
-    static Type *T_vec64 = FixedVectorType::get(T_int32, 2);
-    static Type *T_vec128 = FixedVectorType::get(T_int32, 4);
     Type *lltype;
     // Short vector should be either 8 bytes or 16 bytes.
     // Note that there are only two distinct fundamental types for
     // short vectors so we normalize them to <2 x i32> and <4 x i32>
     switch (jl_datatype_size(dt)) {
     case 8:
-        lltype = T_vec64;
+        lltype = FixedVectorType::get(Type::getInt32Ty(ctx), 2);
         break;
     case 16:
-        lltype = T_vec128;
+        lltype = FixedVectorType::get(Type::getInt32Ty(ctx), 4);
         break;
     default:
         return nullptr;
@@ -59,7 +57,7 @@
 }
 
 #define jl_is_floattype(v)   jl_subtype(v,(jl_value_t*)jl_floatingpoint_type)
-Type *get_llvm_fptype(jl_datatype_t *dt) const
+Type *get_llvm_fptype(jl_datatype_t *dt, LLVMContext &ctx) const
 {
     // Assume jl_is_datatype(dt) && !jl_is_abstracttype(dt)
     // `!dt->name->mutabl && dt->pointerfree && !dt->haspadding && dt->nfields == 0`
@@ -67,16 +65,16 @@
     // Check size first since it's cheaper.
     switch (jl_datatype_size(dt)) {
     case 2:
-        lltype = T_float16;
+        lltype = Type::getHalfTy(ctx);
         break;
     case 4:
-        lltype = T_float32;
+        lltype = Type::getFloatTy(ctx);
         break;
     case 8:
-        lltype = T_float64;
+        lltype = Type::getDoubleTy(ctx);
         break;
     case 16:
-        lltype = T_float128;
+        lltype = Type::getFP128Ty(ctx);
         break;
     default:
         return nullptr;
@@ -85,12 +83,12 @@
             lltype : nullptr);
 }
 
-Type *get_llvm_fp_or_vectype(jl_datatype_t *dt) const
+Type *get_llvm_fp_or_vectype(jl_datatype_t *dt, LLVMContext &ctx) const
 {
     // Assume jl_is_datatype(dt) && !jl_is_abstracttype(dt)
     if (dt->name->mutabl || dt->layout->npointers || dt->layout->haspadding)
         return nullptr;
-    return dt->layout->nfields ? get_llvm_vectype(dt) : get_llvm_fptype(dt);
+    return dt->layout->nfields ? get_llvm_vectype(dt, ctx) : get_llvm_fptype(dt, ctx);
 }
 
 struct ElementType {
@@ -105,7 +103,7 @@
 // Data Types of the members that compose the type are the same.
 // Note that it is the fundamental types that are important and not the member
 // types.
-bool isHFAorHVA(jl_datatype_t *dt, size_t dsz, size_t &nele, ElementType &ele) const
+bool isHFAorHVA(jl_datatype_t *dt, size_t dsz, size_t &nele, ElementType &ele, LLVMContext &ctx) const
 {
     // Assume:
     //     dt is a pointerfree type, (all members are isbits)
@@ -133,7 +131,7 @@
             dt = (jl_datatype_t*)jl_field_type(dt, i);
             continue;
         }
-        if (Type *vectype = get_llvm_vectype(dt)) {
+        if (Type *vectype = get_llvm_vectype(dt, ctx)) {
             if ((ele.sz && dsz != ele.sz) || (ele.type && ele.type != vectype))
                 return false;
             ele.type = vectype;
@@ -149,7 +147,7 @@
             jl_datatype_t *fieldtype = (jl_datatype_t*)jl_field_type(dt, i);
             // Check element count.
             // This needs to be done after the zero size member check
-            if (nele > 3 || !isHFAorHVA(fieldtype, fieldsz, nele, ele)) {
+            if (nele > 3 || !isHFAorHVA(fieldtype, fieldsz, nele, ele, ctx)) {
                 return false;
             }
         }
@@ -158,7 +156,7 @@
     // For bitstypes
     if (ele.sz && dsz != ele.sz)
         return false;
-    Type *new_type = get_llvm_fptype(dt);
+    Type *new_type = get_llvm_fptype(dt, ctx);
     if (new_type && (!ele.type || ele.type == new_type)) {
         ele.type = new_type;
         ele.sz = dsz;
@@ -168,7 +166,7 @@
     return false;
 }
 
-Type *isHFAorHVA(jl_datatype_t *dt, size_t &nele) const
+Type *isHFAorHVA(jl_datatype_t *dt, size_t &nele, LLVMContext &ctx) const
 {
     // Assume jl_is_datatype(dt) && !jl_is_abstracttype(dt)
 
@@ -184,18 +182,18 @@
         return NULL;
     nele = 0;
     ElementType eltype;
-    if (isHFAorHVA(dt, dsz, nele, eltype))
+    if (isHFAorHVA(dt, dsz, nele, eltype, ctx))
         return eltype.type;
     return NULL;
 }
 
-bool needPassByRef(jl_datatype_t *dt, AttrBuilder &ab) override
+bool needPassByRef(jl_datatype_t *dt, AttrBuilder &ab, LLVMContext &ctx, Type *Ty) override
 {
     // B.2
     //   If the argument type is an HFA or an HVA, then the argument is used
     //   unmodified.
     size_t size;
-    if (isHFAorHVA(dt, size))
+    if (isHFAorHVA(dt, size, ctx))
         return false;
     // B.3
     //   If the argument type is a Composite Type that is larger than 16 bytes,
@@ -222,7 +220,7 @@
 //
 // All the out parameters should be default to `false`.
 Type *classify_arg(jl_datatype_t *dt, bool *fpreg, bool *onstack,
-                   size_t *rewrite_len) const
+                   size_t *rewrite_len, LLVMContext &ctx) const
 {
     // Based on section 5.4 C of the Procedure Call Standard
     // C.1
@@ -231,7 +229,7 @@
     //   the argument is allocated to the least significant bits of register
     //   v[NSRN]. The NSRN is incremented by one. The argument has now been
     //   allocated.
-    if (get_llvm_fp_or_vectype(dt)) {
+    if (get_llvm_fp_or_vectype(dt, ctx)) {
         *fpreg = true;
         return NULL;
     }
@@ -243,7 +241,7 @@
     //   Floating-point Registers (with one register per member of the HFA
     //   or HVA). The NSRN is incremented by the number of registers used.
     //   The argument has now been allocated.
-    if (Type *eltype = isHFAorHVA(dt, *rewrite_len)) {
+    if (Type *eltype = isHFAorHVA(dt, *rewrite_len, ctx)) {
         assert(*rewrite_len > 0 && *rewrite_len <= 4);
         // HFA and HVA have <= 4 members
         *fpreg = true;
@@ -322,7 +320,7 @@
     assert(jl_datatype_size(dt) <= 16); // Should be pass by reference otherwise
     *rewrite_len = (jl_datatype_size(dt) + 7) >> 3;
     // Rewrite to [n x Int64] where n is the **size in dword**
-    return jl_datatype_size(dt) ? T_int64 : NULL;
+    return jl_datatype_size(dt) ? Type::getInt64Ty(ctx) : NULL;
 
     // C.11
     //   The NGRN is set to 8.
@@ -346,7 +344,7 @@
     // <handled by C.10 above>
 }
 
-bool use_sret(jl_datatype_t *dt) override
+bool use_sret(jl_datatype_t *dt, LLVMContext &ctx) override
 {
     // Section 5.5
     // If the type, T, of the result of a function is such that
@@ -360,18 +358,18 @@
     bool fpreg = false;
     bool onstack = false;
     size_t rewrite_len = 0;
-    classify_arg(dt, &fpreg, &onstack, &rewrite_len);
+    classify_arg(dt, &fpreg, &onstack, &rewrite_len, ctx);
     return onstack;
 }
 
-Type *preferred_llvm_type(jl_datatype_t *dt, bool isret) const override
+Type *preferred_llvm_type(jl_datatype_t *dt, bool isret, LLVMContext &ctx) const override
 {
-    if (Type *fptype = get_llvm_fp_or_vectype(dt))
+    if (Type *fptype = get_llvm_fp_or_vectype(dt, ctx))
         return fptype;
     bool fpreg = false;
     bool onstack = false;
     size_t rewrite_len = 0;
-    if (Type *rewrite_ty = classify_arg(dt, &fpreg, &onstack, &rewrite_len))
+    if (Type *rewrite_ty = classify_arg(dt, &fpreg, &onstack, &rewrite_len, ctx))
         return ArrayType::get(rewrite_ty, rewrite_len);
     return NULL;
 }
diff -Naur julia-1.7.3/src/abi_arm.cpp julia-1.7.3.new/src/abi_arm.cpp
--- julia-1.7.3/src/abi_arm.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/abi_arm.cpp	2022-07-16 20:42:45.791771286 +0200
@@ -23,14 +23,14 @@
 
 struct ABI_ARMLayout : AbiLayout {
 
-bool needPassByRef(jl_datatype_t *dt, AttrBuilder &ab) override
+bool needPassByRef(jl_datatype_t *dt, AttrBuilder &abi, LLVMContext &ctx, Type *Ty) override
 {
     return false;
 }
 
 #define jl_is_floattype(v)   jl_subtype(v,(jl_value_t*)jl_floatingpoint_type)
 
-Type *get_llvm_fptype(jl_datatype_t *dt) const
+Type *get_llvm_fptype(jl_datatype_t *dt, LLVMContext &ctx) const
 {
     // Assume jl_is_datatype(dt) && !jl_is_abstracttype(dt)
     if (dt->name->mutabl || jl_datatype_nfields(dt) != 0)
@@ -39,13 +39,13 @@
     // Check size first since it's cheaper.
     switch (jl_datatype_size(dt)) {
     case 2:
-        lltype = T_float16;
+        lltype = Type::getHalfTy(ctx);
         break;
     case 4:
-        lltype = T_float32;
+        lltype = Type::getFloatTy(ctx);
         break;
     case 8:
-        lltype = T_float64;
+        lltype = Type::getDoubleTy(ctx);
         break;
     default:
         return NULL;
@@ -58,10 +58,10 @@
 // fundamental type.
 //
 // Returns the corresponding LLVM type.
-Type *isLegalHAType(jl_datatype_t *dt) const
+Type *isLegalHAType(jl_datatype_t *dt, LLVMContext &ctx) const
 {
     // single- or double-precision floating-point type
-    if (Type *fp = get_llvm_fptype(dt))
+    if (Type *fp = get_llvm_fptype(dt, ctx))
         return fp;
 
     // NOT SUPPORTED: 64- or 128-bit containerized vectors
@@ -74,7 +74,7 @@
 //
 // Legality of the HA is determined by a nonzero return value.
 // In case of a non-legal HA, the value of 'base' is undefined.
-size_t isLegalHA(jl_datatype_t *dt, Type *&base) const
+size_t isLegalHA(jl_datatype_t *dt, Type *&base, LLVMContext &ctx) const
 {
     // Homogeneous aggregates are only used for VFP registers,
     // so use that definition of legality (section 6.1.2.1)
@@ -92,10 +92,10 @@
         for (size_t i = 0; i < parent_members; ++i) {
             jl_datatype_t *fdt = (jl_datatype_t*)jl_field_type(dt,i);
 
-            Type *T = isLegalHAType(fdt);
+            Type *T = isLegalHAType(fdt, ctx);
             if (T)
                 total_members++;
-            else if (size_t field_members = isLegalHA(fdt, T))
+            else if (size_t field_members = isLegalHA(fdt, T, ctx))
                 // recursive application (expanding nested composite types)
                 total_members += field_members;
             else
@@ -120,7 +120,7 @@
 // Determine if an argument can be passed through a coprocessor register.
 //
 // All the out parameters should be default to `false`.
-void classify_cprc(jl_datatype_t *dt, bool *vfp) const
+void classify_cprc(jl_datatype_t *dt, bool *vfp, LLVMContext &ctx) const
 {
     // Based on section 6.1 of the Procedure Call Standard
 
@@ -128,7 +128,7 @@
     // - A half-precision floating-point type.
     // - A single-precision floating-point type.
     // - A double-precision floating-point type.
-    if (get_llvm_fptype(dt)) {
+    if (get_llvm_fptype(dt, ctx)) {
         *vfp = true;
         return;
     }
@@ -137,14 +137,14 @@
 
     // - A Homogeneous Aggregate
     Type *base = NULL;
-    if (isLegalHA(dt, base)) {
+    if (isLegalHA(dt, base, ctx)) {
         *vfp = true;
         return;
     }
 }
 
-void classify_return_arg(jl_datatype_t *dt, bool *reg,
-                         bool *onstack, bool *need_rewrite) const
+void classify_return_arg(jl_datatype_t *dt, bool *reg, bool *onstack,
+                         bool *need_rewrite, LLVMContext &ctx) const
 {
     // Based on section 5.4 of the Procedure Call Standard
 
@@ -152,7 +152,7 @@
     //   Any result whose type would satisfy the conditions for a VFP CPRC is
     //   returned in the appropriate number of consecutive VFP registers
     //   starting with the lowest numbered register (s0, d0, q0).
-    classify_cprc(dt, reg);
+    classify_cprc(dt, reg, ctx);
     if (*reg)
         return;
 
@@ -196,12 +196,12 @@
         *onstack = true;
 }
 
-bool use_sret(jl_datatype_t *dt) override
+bool use_sret(jl_datatype_t *dt, LLVMContext &ctx) override
 {
     bool reg = false;
     bool onstack = false;
     bool need_rewrite = false;
-    classify_return_arg(dt, &reg, &onstack, &need_rewrite);
+    classify_return_arg(dt, &reg, &onstack, &need_rewrite, ctx);
 
     return onstack;
 }
@@ -218,7 +218,7 @@
 //
 // All the out parameters should be default to `false`.
 void classify_arg(jl_datatype_t *dt, bool *reg,
-                  bool *onstack, bool *need_rewrite) const
+                  bool *onstack, bool *need_rewrite, LLVMContext &ctx) const
 {
     // Based on section 5.5 of the Procedure Call Standard
 
@@ -226,7 +226,7 @@
     //   If the argument is a CPRC and there are sufficient unallocated
     //   co-processor registers of the appropriate class, the argument is
     //   allocated to co-processor registers.
-    classify_cprc(dt, reg);
+    classify_cprc(dt, reg, ctx);
     if (*reg)
         return;
 
@@ -239,18 +239,18 @@
     *need_rewrite = true;
 }
 
-Type *preferred_llvm_type(jl_datatype_t *dt, bool isret) const override
+Type *preferred_llvm_type(jl_datatype_t *dt, bool isret, LLVMContext &ctx) const override
 {
-    if (Type *fptype = get_llvm_fptype(dt))
+    if (Type *fptype = get_llvm_fptype(dt, ctx))
         return fptype;
 
     bool reg = false;
     bool onstack = false;
     bool need_rewrite = false;
     if (isret)
-        classify_return_arg(dt, &reg, &onstack, &need_rewrite);
+        classify_return_arg(dt, &reg, &onstack, &need_rewrite, ctx);
     else
-        classify_arg(dt, &reg, &onstack, &need_rewrite);
+        classify_arg(dt, &reg, &onstack, &need_rewrite, ctx);
 
     if (!need_rewrite)
         return NULL;
@@ -276,7 +276,7 @@
     if (align > 8)
         align = 8;
 
-    Type *T = Type::getIntNTy(jl_LLVMContext, align*8);
+    Type *T = Type::getIntNTy(ctx, align*8);
     return ArrayType::get(T, (jl_datatype_size(dt) + align - 1) / align);
 }
 
diff -Naur julia-1.7.3/src/abi_llvm.cpp julia-1.7.3.new/src/abi_llvm.cpp
--- julia-1.7.3/src/abi_llvm.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/abi_llvm.cpp	2022-07-16 20:42:45.791771286 +0200
@@ -40,17 +40,17 @@
 
 struct ABI_LLVMLayout : AbiLayout {
 
-bool use_sret(jl_datatype_t *ty) override
+bool use_sret(jl_datatype_t *ty, LLVMContext &ctx) override
 {
     return false;
 }
 
-bool needPassByRef(jl_datatype_t *ty, AttrBuilder &ab) override
+bool needPassByRef(jl_datatype_t *ty, AttrBuilder &ab, LLVMContext &ctx, Type *Ty) override
 {
     return false;
 }
 
-Type *preferred_llvm_type(jl_datatype_t *ty, bool isret) const override
+Type *preferred_llvm_type(jl_datatype_t *ty, bool isret, LLVMContext &ctx) const override
 {
     return NULL;
 }
diff -Naur julia-1.7.3/src/abi_ppc64le.cpp julia-1.7.3.new/src/abi_ppc64le.cpp
--- julia-1.7.3/src/abi_ppc64le.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/abi_ppc64le.cpp	2022-07-16 20:42:45.792771231 +0200
@@ -92,7 +92,7 @@
     return n;
 }
 
-bool use_sret(jl_datatype_t *dt) override
+bool use_sret(jl_datatype_t *dt, LLVMContext &ctx) override
 {
     jl_datatype_t *ty0 = NULL;
     bool hva = false;
@@ -101,18 +101,22 @@
     return false;
 }
 
-bool needPassByRef(jl_datatype_t *dt, AttrBuilder &ab) override
+bool needPassByRef(jl_datatype_t *dt, AttrBuilder &ab, LLVMContext &ctx, Type *Ty) override
 {
     jl_datatype_t *ty0 = NULL;
     bool hva = false;
     if (jl_datatype_size(dt) > 64 && isHFA(dt, &ty0, &hva) > 8) {
+#if JL_LLVM_VERSION < 120000
         ab.addAttribute(Attribute::ByVal);
+#else
+        ab.addByValAttr(Ty);
+#endif
         return true;
     }
     return false;
 }
 
-Type *preferred_llvm_type(jl_datatype_t *dt, bool isret) const override
+Type *preferred_llvm_type(jl_datatype_t *dt, bool isret, LLVMContext &ctx) const override
 {
     // Arguments are either scalar or passed by value
     size_t size = jl_datatype_size(dt);
@@ -142,14 +146,15 @@
     // the bitsize of the integer gives the desired alignment
     if (size > 8) {
         if (jl_datatype_align(dt) <= 8) {
+            Type  *T_int64 = Type::getInt64Ty(ctx);
             return ArrayType::get(T_int64, (size + 7) / 8);
         }
         else {
-            Type *T_int128 = Type::getIntNTy(jl_LLVMContext, 128);
+            Type *T_int128 = Type::getIntNTy(ctx, 128);
             return ArrayType::get(T_int128, (size + 15) / 16);
         }
     }
-    return Type::getIntNTy(jl_LLVMContext, size * 8);
+    return Type::getIntNTy(ctx, size * 8);
 }
 
 };
diff -Naur julia-1.7.3/src/abi_win32.cpp julia-1.7.3.new/src/abi_win32.cpp
--- julia-1.7.3/src/abi_win32.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/abi_win32.cpp	2022-07-16 20:42:45.792771231 +0200
@@ -39,7 +39,7 @@
 
 struct ABI_Win32Layout : AbiLayout {
 
-bool use_sret(jl_datatype_t *dt) override
+bool use_sret(jl_datatype_t *dt, LLVMContext &ctx) override
 {
     // Use sret if the size of the argument is not one of 1, 2, 4, 8 bytes
     // This covers the special case of ComplexF32
@@ -49,23 +49,27 @@
     return true;
 }
 
-bool needPassByRef(jl_datatype_t *dt, AttrBuilder &ab) override
+bool needPassByRef(jl_datatype_t *dt, AttrBuilder &ab, LLVMContext &ctx, Type *Ty) override
 {
     // Use pass by reference for all structs
     if (dt->layout->nfields > 0) {
+#if JL_LLVM_VERSION < 120000
         ab.addAttribute(Attribute::ByVal);
+#else
+        ab.addByValAttr(Ty);
+#endif
         return true;
     }
     return false;
 }
 
-Type *preferred_llvm_type(jl_datatype_t *dt, bool isret) const override
+Type *preferred_llvm_type(jl_datatype_t *dt, bool isret, LLVMContext &ctx) const override
 {
     // Arguments are either scalar or passed by value
     // rewrite integer sized (non-sret) struct to the corresponding integer
     if (!dt->layout->nfields)
         return NULL;
-    return Type::getIntNTy(jl_LLVMContext, jl_datatype_nbits(dt));
+    return Type::getIntNTy(ctx, jl_datatype_nbits(dt));
 }
 
 };
diff -Naur julia-1.7.3/src/abi_win64.cpp julia-1.7.3.new/src/abi_win64.cpp
--- julia-1.7.3/src/abi_win64.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/abi_win64.cpp	2022-07-16 20:42:45.792771231 +0200
@@ -47,7 +47,7 @@
 int nargs;
 ABI_Win64Layout() : nargs(0) { }
 
-bool use_sret(jl_datatype_t *dt) override
+bool use_sret(jl_datatype_t *dt, LLVMContext &ctx) override
 {
     size_t size = jl_datatype_size(dt);
     if (win64_reg_size(size) || is_native_simd_type(dt))
@@ -56,22 +56,27 @@
     return true;
 }
 
-bool needPassByRef(jl_datatype_t *dt, AttrBuilder &ab) override
+bool needPassByRef(jl_datatype_t *dt, AttrBuilder &ab, LLVMContext &ctx, Type *Ty) override
 {
     nargs++;
     size_t size = jl_datatype_size(dt);
     if (win64_reg_size(size))
         return false;
-    if (nargs <= 4)
+    if (nargs <= 4) {
+#if JL_LLVM_VERSION < 120000
         ab.addAttribute(Attribute::ByVal);
+#else
+        ab.addByValAttr(Ty);
+#endif
+    }
     return true;
 }
 
-Type *preferred_llvm_type(jl_datatype_t *dt, bool isret) const override
+Type *preferred_llvm_type(jl_datatype_t *dt, bool isret, LLVMContext &ctx) const override
 {
     size_t size = jl_datatype_size(dt);
     if (size > 0 && win64_reg_size(size) && !jl_is_primitivetype(dt))
-        return Type::getIntNTy(jl_LLVMContext, jl_datatype_nbits(dt));
+        return Type::getIntNTy(ctx, jl_datatype_nbits(dt));
     return NULL;
 }
 
diff -Naur julia-1.7.3/src/abi_x86_64.cpp julia-1.7.3.new/src/abi_x86_64.cpp
--- julia-1.7.3/src/abi_x86_64.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/abi_x86_64.cpp	2022-07-16 20:42:45.793771176 +0200
@@ -168,7 +168,7 @@
     return cl;
 }
 
-bool use_sret(jl_datatype_t *dt) override
+bool use_sret(jl_datatype_t *dt, LLVMContext &ctx) override
 {
     int sret = classify(dt).isMemory;
     if (sret) {
@@ -178,11 +178,15 @@
     return sret;
 }
 
-bool needPassByRef(jl_datatype_t *dt, AttrBuilder &ab) override
+bool needPassByRef(jl_datatype_t *dt, AttrBuilder &ab, LLVMContext &ctx, Type *Ty) override
 {
     Classification cl = classify(dt);
     if (cl.isMemory) {
+#if JL_LLVM_VERSION < 120000
         ab.addAttribute(Attribute::ByVal);
+#else
+        ab.addByValAttr(Ty);
+#endif
         return true;
     }
 
@@ -202,7 +206,12 @@
     else if (jl_is_structtype(dt)) {
         // spill to memory even though we would ordinarily pass
         // it in registers
+#if JL_LLVM_VERSION < 120000
         ab.addAttribute(Attribute::ByVal);
+#else
+        Type* Ty = preferred_llvm_type(dt, false, ctx);
+        ab.addByValAttr(Ty);
+#endif
         return true;
     }
     return false;
@@ -210,7 +219,7 @@
 
 // Called on behalf of ccall to determine preferred LLVM representation
 // for an argument or return value.
-Type *preferred_llvm_type(jl_datatype_t *dt, bool isret) const override
+Type *preferred_llvm_type(jl_datatype_t *dt, bool isret, LLVMContext &ctx) const override
 {
     (void) isret;
     // no need to rewrite these types (they are returned as pointers anyways)
@@ -230,15 +239,15 @@
     switch (cl.classes[0]) {
         case Integer:
             if (size >= 8)
-                types[0] = T_int64;
+                types[0] = Type::getInt64Ty(ctx);
             else
-                types[0] = Type::getIntNTy(jl_LLVMContext, nbits);
+                types[0] = Type::getIntNTy(ctx, nbits);
             break;
         case Sse:
             if (size <= 4)
-                types[0] = T_float32;
+                types[0] = Type::getFloatTy(ctx);
             else
-                types[0] = T_float64;
+                types[0] = Type::getDoubleTy(ctx);
             break;
         default:
             assert(0 && "Unexpected cl.classes[0]");
@@ -248,14 +257,14 @@
             return types[0];
         case Integer:
             assert(size > 8);
-            types[1] = Type::getIntNTy(jl_LLVMContext, (nbits-64));
-            return StructType::get(jl_LLVMContext,ArrayRef<Type*>(&types[0],2));
+            types[1] = Type::getIntNTy(ctx, (nbits-64));
+            return StructType::get(ctx,ArrayRef<Type*>(&types[0],2));
         case Sse:
             if (size <= 12)
-                types[1] = T_float32;
+                types[1] = Type::getFloatTy(ctx);
             else
-                types[1] = T_float64;
-            return StructType::get(jl_LLVMContext,ArrayRef<Type*>(&types[0],2));
+                types[1] = Type::getDoubleTy(ctx);
+            return StructType::get(ctx,ArrayRef<Type*>(&types[0],2));
         default:
             assert(0 && "Unexpected cl.classes[0]");
     }
diff -Naur julia-1.7.3/src/abi_x86.cpp julia-1.7.3.new/src/abi_x86.cpp
--- julia-1.7.3/src/abi_x86.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/abi_x86.cpp	2022-07-16 20:42:45.793771176 +0200
@@ -57,7 +57,7 @@
     return is_complex_type(dt) && jl_tparam0(dt) == (jl_value_t*)jl_float64_type;
 }
 
-bool use_sret(jl_datatype_t *dt) override
+bool use_sret(jl_datatype_t *dt, LLVMContext &ctx) override
 {
     size_t size = jl_datatype_size(dt);
     if (size == 0)
@@ -67,16 +67,20 @@
     return true;
 }
 
-bool needPassByRef(jl_datatype_t *dt, AttrBuilder &ab) override
+bool needPassByRef(jl_datatype_t *dt, AttrBuilder &ab, LLVMContext &ctx, Type *Ty) override
 {
     size_t size = jl_datatype_size(dt);
     if (is_complex64(dt) || is_complex128(dt) || (jl_is_primitivetype(dt) && size <= 8))
         return false;
-    ab.addAttribute(Attribute::ByVal);
+#if JL_LLVM_VERSION < 120000
+        ab.addAttribute(Attribute::ByVal);
+#else
+        ab.addByValAttr(Ty);
+#endif
     return true;
 }
 
-Type *preferred_llvm_type(jl_datatype_t *dt, bool isret) const override
+Type *preferred_llvm_type(jl_datatype_t *dt, bool isret, LLVMContext &ctx) const override
 {
     if (!isret)
         return NULL;
diff -Naur julia-1.7.3/src/aotcompile.cpp julia-1.7.3.new/src/aotcompile.cpp
--- julia-1.7.3/src/aotcompile.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/aotcompile.cpp	2022-07-16 20:42:45.793771176 +0200
@@ -569,6 +569,10 @@
     std::unique_ptr<Module> sysimage(new Module("sysimage", Context));
     sysimage->setTargetTriple(data->M->getTargetTriple());
     sysimage->setDataLayout(data->M->getDataLayout());
+#if JL_LLVM_VERSION >= 130000
+    sysimage->setStackProtectorGuard(data->M->getStackProtectorGuard());
+    sysimage->setOverrideStackAlignment(data->M->getOverrideStackAlignment());
+#endif
     data->M.reset(); // free memory for data->M
 
     if (sysimg_data) {
diff -Naur julia-1.7.3/src/ccall.cpp julia-1.7.3.new/src/ccall.cpp
--- julia-1.7.3/src/ccall.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/ccall.cpp	2022-07-16 20:42:45.794771121 +0200
@@ -290,9 +290,9 @@
 class AbiLayout {
 public:
     virtual ~AbiLayout() {}
-    virtual bool use_sret(jl_datatype_t *ty) = 0;
-    virtual bool needPassByRef(jl_datatype_t *ty, AttrBuilder&) = 0;
-    virtual Type *preferred_llvm_type(jl_datatype_t *ty, bool isret) const = 0;
+    virtual bool use_sret(jl_datatype_t *ty, LLVMContext &ctx) = 0;
+    virtual bool needPassByRef(jl_datatype_t *ty, AttrBuilder&, LLVMContext &ctx, Type* llvm_t) = 0;
+    virtual Type *preferred_llvm_type(jl_datatype_t *ty, bool isret, LLVMContext &ctx) const = 0;
 };
 
 // Determine if object of bitstype ty maps to a native x86 SIMD type (__m128, __m256, or __m512) in C
@@ -882,6 +882,10 @@
     // copy module properties that should always match
     Mod->setTargetTriple(jl_Module->getTargetTriple());
     Mod->setDataLayout(jl_Module->getDataLayout());
+#if JL_LLVM_VERSION >= 130000
+    Mod->setStackProtectorGuard(jl_Module->getStackProtectorGuard());
+    Mod->setOverrideStackAlignment(jl_Module->getOverrideStackAlignment());
+#endif
 
     // verify the definition
     Function *def = Mod->getFunction(ir_name);
@@ -1008,14 +1012,14 @@
 
     if (type_is_ghost(lrt)) {
         prt = lrt = T_void;
-        abi->use_sret(jl_nothing_type);
+        abi->use_sret(jl_nothing_type, jl_LLVMContext);
     }
     else {
         if (!jl_is_datatype(rt) || ((jl_datatype_t*)rt)->layout == NULL || jl_is_layout_opaque(((jl_datatype_t*)rt)->layout) || jl_is_cpointer_type(rt) || retboxed) {
             prt = lrt; // passed as pointer
-            abi->use_sret(jl_voidpointer_type);
+            abi->use_sret(jl_voidpointer_type, jl_LLVMContext);
         }
-        else if (abi->use_sret((jl_datatype_t*)rt)) {
+        else if (abi->use_sret((jl_datatype_t*)rt, jl_LLVMContext)) {
             AttrBuilder retattrs = AttrBuilder();
 #if !defined(_OS_WINDOWS_) // llvm used to use the old mingw ABI, skipping this marking works around that difference
 #if JL_LLVM_VERSION < 120000
@@ -1031,7 +1035,7 @@
             prt = lrt;
         }
         else {
-            prt = abi->preferred_llvm_type((jl_datatype_t*)rt, true);
+            prt = abi->preferred_llvm_type((jl_datatype_t*)rt, true, jl_LLVMContext);
             if (prt == NULL)
                 prt = lrt;
         }
@@ -1077,7 +1081,7 @@
         }
 
         // Whether or not LLVM wants us to emit a pointer to the data
-        bool byRef = abi->needPassByRef((jl_datatype_t*)tti, ab);
+        bool byRef = abi->needPassByRef((jl_datatype_t*)tti, ab, jl_LLVMContext, t);
 
         if (jl_is_cpointer_type(tti)) {
             pat = t;
@@ -1086,7 +1090,7 @@
             pat = PointerType::get(t, AddressSpace::Derived);
         }
         else {
-            pat = abi->preferred_llvm_type((jl_datatype_t*)tti, false);
+            pat = abi->preferred_llvm_type((jl_datatype_t*)tti, false, jl_LLVMContext);
             if (pat == NULL)
                 pat = t;
         }
diff -Naur julia-1.7.3/src/cgutils.cpp julia-1.7.3.new/src/cgutils.cpp
--- julia-1.7.3/src/cgutils.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/cgutils.cpp	2022-07-16 20:42:45.794771121 +0200
@@ -2,14 +2,6 @@
 
 // utility procedures used in code generation
 
-static Instruction *tbaa_decorate(MDNode *md, Instruction *inst)
-{
-    inst->setMetadata(llvm::LLVMContext::MD_tbaa, md);
-    if (isa<LoadInst>(inst) && md == tbaa_const)
-        inst->setMetadata(LLVMContext::MD_invariant_load, MDNode::get(md->getContext(), None));
-    return inst;
-}
-
 static Value *track_pjlvalue(jl_codectx_t &ctx, Value *V)
 {
     assert(V->getType() == T_pjlvalue);
@@ -2762,8 +2754,14 @@
     size_t nargs;
     if (const auto *CC = dyn_cast<ConstantAggregate>(constant))
         nargs = CC->getNumOperands();
-    else if (const auto *CAZ = dyn_cast<ConstantAggregateZero>(constant))
+    else if (const auto *CAZ = dyn_cast<ConstantAggregateZero>(constant)) {
+#if JL_LLVM_VERSION >= 130000
+        // SVE: Elsewhere we use `getMinKownValue`
+        nargs = CAZ->getElementCount().getFixedValue();
+#else
         nargs = CAZ->getNumElements();
+#endif
+    }
     else if (const auto *CDS = dyn_cast<ConstantDataSequential>(constant))
         nargs = CDS->getNumElements();
     else
@@ -3209,9 +3207,9 @@
 // allocation for known size object
 static Value *emit_allocobj(jl_codectx_t &ctx, size_t static_size, Value *jt)
 {
-    Value *ptls_ptr = emit_bitcast(ctx, get_current_ptls(ctx), T_pint8);
+    Value *current_task = get_current_task(ctx);
     Function *F = prepare_call(jl_alloc_obj_func);
-    auto call = ctx.builder.CreateCall(F, {ptls_ptr, ConstantInt::get(T_size, static_size), maybe_decay_untracked(ctx, jt)});
+    auto call = ctx.builder.CreateCall(F, {current_task, ConstantInt::get(T_size, static_size), maybe_decay_untracked(ctx, jt)});
     call->setAttributes(F->getAttributes());
     return call;
 }
diff -Naur julia-1.7.3/src/codegen.cpp julia-1.7.3.new/src/codegen.cpp
--- julia-1.7.3/src/codegen.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/codegen.cpp	2022-07-16 20:42:45.795771066 +0200
@@ -156,7 +156,7 @@
 bool imaging_mode = false;
 
 // shared llvm state
-JL_DLLEXPORT LLVMContext &jl_LLVMContext = *(new LLVMContext());
+static LLVMContext &jl_LLVMContext = *(new LLVMContext());
 TargetMachine *jl_TargetMachine;
 static DataLayout &jl_data_layout = *(new DataLayout(""));
 #define jl_Module ctx.f->getParent()
@@ -641,7 +641,7 @@
 static const auto jl_alloc_obj_func = new JuliaFunction{
     "julia.gc_alloc_obj",
     [](LLVMContext &C) { return FunctionType::get(T_prjlvalue,
-                {T_pint8, T_size, T_prjlvalue}, false); },
+                {T_ppjlvalue, T_size, T_prjlvalue}, false); },
     [](LLVMContext &C) { return AttributeList::get(C,
             AttributeSet::get(C, makeArrayRef({Attribute::getWithAllocSizeArgs(C, 1, None)})), // returns %1 bytes
             Attributes(C, {Attribute::NoAlias, Attribute::NonNull}),
@@ -1171,7 +1171,7 @@
 
 static Value *literal_pointer_val(jl_codectx_t &ctx, jl_value_t *p);
 static GlobalVariable *prepare_global_in(Module *M, GlobalVariable *G);
-static Instruction *tbaa_decorate(MDNode *md, Instruction *inst);
+Instruction *tbaa_decorate(MDNode *md, Instruction *inst);
 
 static GlobalVariable *prepare_global_in(Module *M, JuliaVariable *G)
 {
@@ -1742,6 +1742,16 @@
             llvm::DEBUG_METADATA_VERSION);
     m->setDataLayout(jl_data_layout);
     m->setTargetTriple(jl_TargetMachine->getTargetTriple().str());
+
+#if defined(_OS_WINDOWS_) && !defined(_CPU_X86_64_) && JL_LLVM_VERSION >= 130000
+    // tell Win32 to assume the stack is always 16-byte aligned,
+    // and to ensure that it is 16-byte aligned for out-going calls,
+    // to ensure compatibility with GCC codes
+    m->setOverrideStackAlignment(16);
+#endif
+#if defined(JL_DEBUG_BUILD) && JL_LLVM_VERSION >= 130000
+    m->setStackProtectorGuard("global");
+#endif
 }
 
 Module *jl_create_llvm_module(StringRef name)
@@ -4986,19 +4996,7 @@
 // Get PTLS through current task.
 static Value *get_current_ptls(jl_codectx_t &ctx)
 {
-    const int ptls_offset = offsetof(jl_task_t, ptls);
-    Value *pptls = ctx.builder.CreateInBoundsGEP(
-        T_pjlvalue, get_current_task(ctx),
-        ConstantInt::get(T_size, ptls_offset / sizeof(void *)),
-        "ptls_field");
-    LoadInst *ptls_load = ctx.builder.CreateAlignedLoad(
-        emit_bitcast(ctx, pptls, T_ppjlvalue), Align(sizeof(void *)), "ptls_load");
-    // Note: Corresponding store (`t->ptls = ptls`) happens in `ctx_switch` of tasks.c.
-    tbaa_decorate(tbaa_gcframe, ptls_load);
-    // Using `CastInst::Create` to get an `Instruction*` without explicit cast:
-    auto ptls = CastInst::Create(Instruction::BitCast, ptls_load, T_ppjlvalue, "ptls");
-    ctx.builder.Insert(ptls);
-    return ptls;
+    return get_current_ptls_from_task(ctx.builder, get_current_task(ctx), tbaa_gcframe);
 }
 
 // Store world age at the entry block of the function. This function should be
@@ -7381,7 +7379,7 @@
 #endif
                 continue;
             }
-            assert(find(pred_begin(PhiBB), pred_end(PhiBB), FromBB) != pred_end(PhiBB)); // consistency check
+            assert(std::find(pred_begin(PhiBB), pred_end(PhiBB), FromBB) != pred_end(PhiBB)); // consistency check
             TerminatorInst *terminator = FromBB->getTerminator();
             if (!terminator->getParent()->getUniqueSuccessor()) {
                 // Can't use `llvm::SplitCriticalEdge` here because
@@ -7895,7 +7893,7 @@
 
 // --- initialization ---
 
-std::pair<MDNode*,MDNode*> tbaa_make_child(const char *name, MDNode *parent=nullptr, bool isConstant=false)
+static std::pair<MDNode*,MDNode*> tbaa_make_child(const char *name, MDNode *parent=nullptr, bool isConstant=false)
 {
     MDBuilder mbuilder(jl_LLVMContext);
     if (tbaa_root == nullptr) {
@@ -8220,13 +8218,14 @@
 
     TargetOptions options = TargetOptions();
     //options.PrintMachineCode = true; //Print machine code produced during JIT compiling
-#if defined(_OS_WINDOWS_) && !defined(_CPU_X86_64_)
+#if defined(_OS_WINDOWS_) && !defined(_CPU_X86_64_) && JL_LLVM_VERSION < 130000
     // tell Win32 to assume the stack is always 16-byte aligned,
     // and to ensure that it is 16-byte aligned for out-going calls,
     // to ensure compatibility with GCC codes
+    // In LLVM 13 and onwards this has turned into a module option
     options.StackAlignmentOverride = 16;
 #endif
-#ifdef JL_DEBUG_BUILD
+#if defined(JL_DEBUG_BUILD) && JL_LLVM_VERSION < 130000
     // LLVM defaults to tls stack guard, which causes issues with Julia's tls implementation
     options.StackProtectorGuard = StackProtectorGuards::Global;
 #endif
diff -Naur julia-1.7.3/src/codegen_shared.h julia-1.7.3.new/src/codegen_shared.h
--- julia-1.7.3/src/codegen_shared.h	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/codegen_shared.h	2022-07-16 20:42:45.796771011 +0200
@@ -5,6 +5,8 @@
 #include <llvm/Support/Debug.h>
 #include <llvm/IR/DebugLoc.h>
 #include <llvm/IR/IRBuilder.h>
+#include <llvm/IR/MDBuilder.h>
+#include "julia.h"
 
 enum AddressSpace {
     Generic = 0,
@@ -16,6 +18,24 @@
     LastSpecial = Loaded,
 };
 
+namespace JuliaType {
+    static inline llvm::StructType* get_jlvalue_ty(llvm::LLVMContext &C) {
+        return llvm::StructType::get(C);
+    }
+
+    static inline llvm::PointerType* get_pjlvalue_ty(llvm::LLVMContext &C) {
+        return llvm::PointerType::get(get_jlvalue_ty(C), 0);
+    }
+
+    static inline llvm::PointerType* get_prjlvalue_ty(llvm::LLVMContext &C) {
+        return llvm::PointerType::get(get_jlvalue_ty(C), AddressSpace::Tracked);
+    }
+
+    static inline llvm::PointerType* get_ppjlvalue_ty(llvm::LLVMContext &C) {
+        return llvm::PointerType::get(get_pjlvalue_ty(C), 0);
+    }
+}
+
 // JLCALL with API arguments ([extra], arg0, arg1, arg2, ...) has the following ABI calling conventions defined:
 #define JLCALL_F_CC (CallingConv::ID)37     // (jl_value_t *arg0, jl_value_t **argv, uint32_t nargv)
 #define JLCALL_F2_CC (CallingConv::ID)38    // (jl_value_t *arg0, jl_value_t **argv, uint32_t nargv, jl_value_t *extra)
@@ -65,3 +85,63 @@
     dbg->print(llvm::dbgs());
     llvm::dbgs() << "\n";
 }
+
+static inline std::pair<llvm::MDNode*,llvm::MDNode*> tbaa_make_child_with_context(llvm::LLVMContext &ctxt, const char *name, llvm::MDNode *parent=nullptr, bool isConstant=false)
+{
+    llvm::MDBuilder mbuilder(ctxt);
+    llvm::MDNode *jtbaa = mbuilder.createTBAARoot("jtbaa");
+    llvm::MDNode *tbaa_root = mbuilder.createTBAAScalarTypeNode("jtbaa", jtbaa);
+    llvm::MDNode *scalar = mbuilder.createTBAAScalarTypeNode(name, parent ? parent : tbaa_root);
+    llvm::MDNode *n = mbuilder.createTBAAStructTagNode(scalar, scalar, 0, isConstant);
+    return std::make_pair(n, scalar);
+}
+
+static inline llvm::MDNode *get_tbaa_const(llvm::LLVMContext &ctxt) {
+    return tbaa_make_child_with_context(ctxt, "jtbaa_const", nullptr, true).first;
+}
+
+static inline llvm::Instruction *tbaa_decorate(llvm::MDNode *md, llvm::Instruction *inst)
+{
+    inst->setMetadata(llvm::LLVMContext::MD_tbaa, md);
+    if (llvm::isa<llvm::LoadInst>(inst) && md && md == get_tbaa_const(md->getContext()))
+        inst->setMetadata(llvm::LLVMContext::MD_invariant_load, llvm::MDNode::get(md->getContext(), llvm::None));
+    return inst;
+}
+
+// bitcast a value, but preserve its address space when dealing with pointer types
+static inline llvm::Value *emit_bitcast_with_builder(llvm::IRBuilder<> &builder, llvm::Value *v, llvm::Type *jl_value)
+{
+    using namespace llvm;
+    if (isa<PointerType>(jl_value) &&
+        v->getType()->getPointerAddressSpace() != jl_value->getPointerAddressSpace()) {
+        // Cast to the proper address space
+        Type *jl_value_addr =
+                PointerType::get(cast<PointerType>(jl_value)->getElementType(),
+                                 v->getType()->getPointerAddressSpace());
+        return builder.CreateBitCast(v, jl_value_addr);
+    }
+    else {
+        return builder.CreateBitCast(v, jl_value);
+    }
+}
+
+// Get PTLS through current task.
+static inline llvm::Value *get_current_ptls_from_task(llvm::IRBuilder<> &builder, llvm::Value *current_task, llvm::MDNode *tbaa)
+{
+    using namespace llvm;
+    auto T_ppjlvalue = JuliaType::get_ppjlvalue_ty(builder.getContext());
+    auto T_size = builder.GetInsertBlock()->getModule()->getDataLayout().getIntPtrType(builder.getContext());
+    const int ptls_offset = offsetof(jl_task_t, ptls);
+    llvm::Value *pptls = builder.CreateInBoundsGEP(
+        JuliaType::get_pjlvalue_ty(builder.getContext()), current_task,
+        ConstantInt::get(T_size, ptls_offset / sizeof(void *)),
+        "ptls_field");
+    LoadInst *ptls_load = builder.CreateAlignedLoad(
+        emit_bitcast_with_builder(builder, pptls, T_ppjlvalue), Align(sizeof(void *)), "ptls_load");
+    // Note: Corresponding store (`t->ptls = ptls`) happens in `ctx_switch` of tasks.c.
+    tbaa_decorate(tbaa, ptls_load);
+    // Using `CastInst::Create` to get an `Instruction*` without explicit cast:
+    auto ptls = CastInst::Create(Instruction::BitCast, ptls_load, T_ppjlvalue, "ptls");
+    builder.Insert(ptls);
+    return ptls;
+}
diff -Naur julia-1.7.3/src/debuginfo.cpp julia-1.7.3.new/src/debuginfo.cpp
--- julia-1.7.3/src/debuginfo.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/debuginfo.cpp	2022-07-16 20:42:45.796771011 +0200
@@ -179,13 +179,6 @@
     { return lhs>rhs; }
 };
 
-struct strrefcomp {
-    bool operator() (const StringRef& lhs, const StringRef& rhs) const
-    {
-        return lhs.compare(rhs) > 0;
-    }
-};
-
 class JuliaJITEventListener: public JITEventListener
 {
     std::map<size_t, ObjectInfo, revcomp> objectmap;
@@ -236,11 +229,13 @@
         SavedObject.second.release();
 
         object::section_iterator EndSection = debugObj.section_end();
-        std::map<StringRef, object::SectionRef, strrefcomp> loadedSections;
+        StringMap<object::SectionRef> loadedSections;
         for (const object::SectionRef &lSection: Object.sections()) {
             auto sName = lSection.getName();
-            if (sName)
-                loadedSections[*sName] = lSection;
+            if (sName) {
+                bool inserted = loadedSections.insert(std::make_pair(*sName, lSection)).second;
+                assert(inserted); (void)inserted;
+            }
         }
         auto getLoadAddress = [&] (const StringRef &sName) -> uint64_t {
             auto search = loadedSections.find(sName);
diff -Naur julia-1.7.3/src/disasm.cpp julia-1.7.3.new/src/disasm.cpp
--- julia-1.7.3/src/disasm.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/disasm.cpp	2022-07-16 20:42:45.796771011 +0200
@@ -800,13 +800,21 @@
     std::unique_ptr<MCRegisterInfo> MRI(TheTarget->createMCRegInfo(TheTriple.str()));
     assert(MRI && "Unable to create target register info!");
 
+    std::unique_ptr<llvm::MCSubtargetInfo> STI(
+      TheTarget->createMCSubtargetInfo(TheTriple.str(), cpu, features));
+    assert(STI && "Unable to create subtarget info!");
+
+#if JL_LLVM_VERSION >= 130000
+    MCContext Ctx(TheTriple, MAI.get(), MRI.get(), STI.get(), &SrcMgr);
+    std::unique_ptr<MCObjectFileInfo> MOFI(
+      TheTarget->createMCObjectFileInfo(Ctx, /*PIC=*/false, /*LargeCodeModel=*/ false));
+    Ctx.setObjectFileInfo(MOFI.get());
+#else
     std::unique_ptr<MCObjectFileInfo> MOFI(new MCObjectFileInfo());
     MCContext Ctx(MAI.get(), MRI.get(), MOFI.get(), &SrcMgr);
     MOFI->InitMCObjectFileInfo(TheTriple, /* PIC */ false, Ctx);
+#endif
 
-    // Set up Subtarget and Disassembler
-    std::unique_ptr<MCSubtargetInfo>
-        STI(TheTarget->createMCSubtargetInfo(TheTriple.str(), cpu, features));
     std::unique_ptr<MCDisassembler> DisAsm(TheTarget->createMCDisassembler(*STI, Ctx));
     if (!DisAsm) {
         rstream << "ERROR: no disassembler for target " << TheTriple.str();
diff -Naur julia-1.7.3/src/jitlayers.cpp julia-1.7.3.new/src/jitlayers.cpp
--- julia-1.7.3/src/jitlayers.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/jitlayers.cpp	2022-07-16 20:42:45.797770955 +0200
@@ -12,6 +12,9 @@
 #include <llvm/Analysis/TargetTransformInfo.h>
 #include <llvm/ExecutionEngine/Orc/CompileUtils.h>
 #include <llvm/ExecutionEngine/Orc/ExecutionUtils.h>
+#if JL_LLVM_VERSION >= 130000
+#include <llvm/ExecutionEngine/Orc/ExecutorProcessControl.h>
+#endif
 #include <llvm/Support/DynamicLibrary.h>
 #include <llvm/Support/FormattedStream.h>
 #include <llvm/Support/SmallVectorMemoryBuffer.h>
@@ -655,7 +658,11 @@
     MemMgr(createRTDyldMemoryManager()),
     JuliaListener(CreateJuliaJITEventListener()),
     TSCtx(std::unique_ptr<LLVMContext>(LLVMCtx)),
+#if JL_LLVM_VERSION >= 130000
+    ES(cantFail(orc::SelfExecutorProcessControl::Create())),
+#else
     ES(),
+#endif
     GlobalJD(ES.createBareJITDylib("JuliaGlobals")),
     JD(ES.createBareJITDylib("JuliaOJIT")),
     ObjectLayer(
@@ -1052,7 +1059,7 @@
             ConstantAggregateZero::get(atype), "__catchjmp") };
     gvs[0]->setSection(".text");
     gvs[1]->setSection(".text");
-    appendToUsed(*m, makeArrayRef((GlobalValue**)gvs, 2));
+    appendToCompilerUsed(*m, makeArrayRef((GlobalValue**)gvs, 2));
 #endif
     jl_jit_share_data(*m);
     assert(jl_ExecutionEngine);
diff -Naur julia-1.7.3/src/llvm-late-gc-lowering.cpp julia-1.7.3.new/src/llvm-late-gc-lowering.cpp
--- julia-1.7.3/src/llvm-late-gc-lowering.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/llvm-late-gc-lowering.cpp	2022-07-16 20:42:45.797770955 +0200
@@ -396,8 +396,14 @@
         }
         if (isa<ArrayType>(T))
             count *= cast<ArrayType>(T)->getNumElements();
-        else if (isa<VectorType>(T))
+        else if (isa<VectorType>(T)) {
+#if JL_LLVM_VERSION >= 120000
+            ElementCount EC = cast<VectorType>(T)->getElementCount();
+            count *= EC.getKnownMinValue();
+#else
             count *= cast<VectorType>(T)->getNumElements();
+#endif
+        }
     }
     if (count == 0)
         all = false;
@@ -408,8 +414,14 @@
         return ST->getNumElements();
     else if (auto *AT = dyn_cast<ArrayType>(T))
         return AT->getNumElements();
-    else
+    else {
+#if JL_LLVM_VERSION >= 120000
+        ElementCount EC = cast<VectorType>(T)->getElementCount();
+        return EC.getKnownMinValue();
+#else
         return cast<VectorType>(T)->getNumElements();
+#endif
+    }
 }
 
 // Walk through a Type, and record the element path to every tracked value inside
@@ -465,6 +477,8 @@
             if (getValueAddrSpace(NewV) == 0)
                 break;
             CurrentV = NewV;
+        } else if (auto *Freeze = dyn_cast<FreezeInst>(CurrentV)) {
+            CurrentV = Freeze->getOperand(0); // Can be formed by optimizations, treat as a no-op
         } else if (auto *GEP = dyn_cast<GetElementPtrInst>(CurrentV)) {
             CurrentV = GEP->getOperand(0);
             // GEP can make vectors from a single base pointer
@@ -604,7 +618,7 @@
     std::vector<Value*> V{Numbers.size()};
     Value *V_rnull = ConstantPointerNull::get(cast<PointerType>(T_prjlvalue));
     for (unsigned i = 0; i < V.size(); ++i) {
-        if (Numbers[i] >= 0)
+        if (Numbers[i] >= 0) // ignores undef and poison values
             V[i] = GetPtrForNumber(S, Numbers[i], InsertBefore);
         else
             V[i] = V_rnull;
@@ -636,8 +650,14 @@
     }
     std::vector<int> Numbers;
     unsigned NumRoots = 1;
-    if (auto VTy = dyn_cast<VectorType>(SI->getType()))
+    if (auto VTy = dyn_cast<VectorType>(SI->getType())) {
+#if JL_LLVM_VERSION >= 120000
+        ElementCount EC = VTy->getElementCount();
+        Numbers.resize(EC.getKnownMinValue(), -1);
+#else
         Numbers.resize(VTy->getNumElements(), -1);
+#endif
+    }
     else
         assert(isa<PointerType>(SI->getType()) && "unimplemented");
     assert(!isTrackedValue(SI));
@@ -691,13 +711,18 @@
         else
             Numbers[i] = Number;
     }
-    if (auto VTy = dyn_cast<VectorType>(SI->getType())) {
+    if (auto VTy = dyn_cast<FixedVectorType>(SI->getType())) {
         if (NumRoots != Numbers.size()) {
             // broadcast the scalar root number to fill the vector
             assert(NumRoots == 1);
             int Number = Numbers[0];
             Numbers.resize(0);
+#if JL_LLVM_VERSION >= 120000
+            ElementCount EC = VTy->getElementCount();
+            Numbers.resize(EC.getKnownMinValue(), Number);
+#else
             Numbers.resize(VTy->getNumElements(), Number);
+#endif
         }
     }
     if (!isa<PointerType>(SI->getType()))
@@ -713,11 +738,12 @@
     SmallVector<PHINode *, 2> lifted;
     std::vector<int> Numbers;
     unsigned NumRoots = 1;
-    if (auto VTy = dyn_cast<VectorType>(Phi->getType())) {
+    if (auto VTy = dyn_cast<FixedVectorType>(Phi->getType())) {
         NumRoots = VTy->getNumElements();
         Numbers.resize(NumRoots);
     }
     else {
+        // TODO: SVE
         assert(isa<PointerType>(Phi->getType()) && "unimplemented");
     }
     for (unsigned i = 0; i < NumRoots; ++i) {
@@ -836,8 +862,9 @@
         std::vector<int> Numbers2 = NumberAll(S, SVI->getOperand(1));
         auto Mask = SVI->getShuffleMask();
         for (auto idx : Mask) {
-            assert(idx != -1 && "Undef tracked value is invalid");
-            if ((unsigned)idx < Numbers1.size()) {
+            if (idx == -1) {
+                Numbers.push_back(-1);
+            } else if ((unsigned)idx < Numbers1.size()) {
                 Numbers.push_back(Numbers1.at(idx));
             } else {
                 Numbers.push_back(Numbers2.at(idx - Numbers1.size()));
@@ -942,8 +969,9 @@
             Number = Numbers[CurrentV.second]; // only needed a subset of the values
             Numbers.resize(tracked.count, Number);
         }
-        else
+        else {
             assert(!isa<PointerType>(V->getType()));
+        }
     }
     if (CurrentV.first != V) {
         if (isa<PointerType>(V->getType())) {
@@ -2284,10 +2312,12 @@
                 // Create a call to the `julia.gc_alloc_bytes` intrinsic, which is like
                 // `julia.gc_alloc_obj` except it doesn't set the tag.
                 auto allocBytesIntrinsic = getOrDeclare(jl_intrinsics::GCAllocBytes);
+                auto ptlsLoad = get_current_ptls_from_task(builder, CI->getArgOperand(0), tbaa_gcframe);
+                auto ptls = builder.CreateBitCast(ptlsLoad, Type::getInt8PtrTy(builder.getContext()));
                 auto newI = builder.CreateCall(
                     allocBytesIntrinsic,
                     {
-                        CI->getArgOperand(0),
+                        ptls,
                         builder.CreateIntCast(
                             CI->getArgOperand(1),
                             allocBytesIntrinsic->getFunctionType()->getParamType(1),
diff -Naur julia-1.7.3/src/llvm-multiversioning.cpp julia-1.7.3.new/src/llvm-multiversioning.cpp
--- julia-1.7.3/src/llvm-multiversioning.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/llvm-multiversioning.cpp	2022-07-16 20:44:02.210557178 +0200
@@ -40,8 +40,6 @@
 
 using namespace llvm;
 
-extern std::pair<MDNode*,MDNode*> tbaa_make_child(const char *name, MDNode *parent=nullptr,
-                                                  bool isConstant=false);
 
 namespace {
 
@@ -344,7 +342,7 @@
       T_int32(Type::getInt32Ty(ctx)),
       T_void(Type::getVoidTy(ctx)),
       T_psize(PointerType::get(T_size, 0)),
-      tbaa_const(tbaa_make_child("jtbaa_const", nullptr, true).first),
+      tbaa_const(tbaa_make_child_with_context(ctx, "jtbaa_const", nullptr, true).first),
       pass(pass),
       specs(jl_get_llvm_clone_targets()),
       fvars(consume_gv<Function>(M, "jl_sysimg_fvars")),
@@ -404,7 +402,12 @@
         vmap[&*J] = &*DestI++;
     }
     SmallVector<ReturnInst*,8> Returns;
+#if JL_LLVM_VERSION >= 130000
+    // We are cloning into the same module
+    CloneFunctionInto(new_f, F, vmap, CloneFunctionChangeType::GlobalChanges, Returns);
+#else
     CloneFunctionInto(new_f, F, vmap, true, Returns);
+#endif
 }
 
 // Clone all clone_all targets. Makes sure that the base targets are all available.
diff -Naur julia-1.7.3/src/llvm-multiversioning.cpp~ julia-1.7.3.new/src/llvm-multiversioning.cpp~
--- julia-1.7.3/src/llvm-multiversioning.cpp~	1970-01-01 01:00:00.000000000 +0100
+++ julia-1.7.3.new/src/llvm-multiversioning.cpp~	2022-07-16 20:42:45.797770955 +0200
@@ -0,0 +1,1148 @@
+// This file is a part of Julia. License is MIT: https://julialang.org/license
+
+// Function multi-versioning
+#define DEBUG_TYPE "julia_multiversioning"
+#undef DEBUG
+
+// LLVM pass to clone function for different archs
+
+#include "llvm-version.h"
+
+#include <llvm-c/Core.h>
+#include <llvm-c/Types.h>
+
+#include <llvm/Pass.h>
+#include <llvm/IR/Module.h>
+#include <llvm/IR/LegacyPassManager.h>
+#include <llvm/IR/Function.h>
+#include <llvm/IR/Instructions.h>
+#include <llvm/IR/Constants.h>
+#include <llvm/IR/LLVMContext.h>
+#include <llvm/Analysis/LoopInfo.h>
+#include <llvm/Analysis/CallGraph.h>
+#include <llvm/IR/LegacyPassManager.h>
+#include <llvm/IR/IRBuilder.h>
+#include <llvm/IR/DebugInfoMetadata.h>
+#include <llvm/Transforms/Utils/Cloning.h>
+
+#include "julia.h"
+#include "julia_internal.h"
+#include "processor.h"
+#include "support/dtypes.h"
+
+#include <map>
+#include <memory>
+#include <set>
+#include <vector>
+
+#include "codegen_shared.h"
+#include "julia_assert.h"
+
+using namespace llvm;
+
+
+namespace {
+
+// These are valid detail cloning conditions in the target flags.
+constexpr uint32_t clone_mask =
+    JL_TARGET_CLONE_LOOP | JL_TARGET_CLONE_SIMD | JL_TARGET_CLONE_MATH;
+
+struct MultiVersioning;
+
+// Treat identical mapping as missing and return `def` in that case.
+// We mainly need this to identify cloned function using value map after LLVM cloning
+// functions fills the map with identity entries.
+template<typename T>
+Value *map_get(T &&vmap, Value *key, Value *def=nullptr)
+{
+    auto val = vmap.lookup(key);
+    if (!val || key == val)
+        return def;
+    return val;
+}
+
+// Iterate through uses of a particular type.
+// Recursively scan through `ConstantExpr` and `ConstantAggregate` use.
+template<typename U>
+struct ConstantUses {
+    template<typename T>
+    struct Info {
+        Use *use;
+        T *val;
+        // If `samebits == true`, the offset the original value appears in the constant.
+        size_t offset;
+        // This specify whether the original value appears in the current value in exactly
+        // the same bit pattern (with possibly an offset determined by `offset`).
+        bool samebits;
+        Info(Use *use, T *val, size_t offset, bool samebits) :
+            use(use),
+            val(val),
+            offset(offset),
+            samebits(samebits)
+        {
+        }
+        Info(Use *use, size_t offset, bool samebits) :
+            use(use),
+            val(cast<T>(use->getUser())),
+            offset(offset),
+            samebits(samebits)
+        {
+        }
+    };
+    using UseInfo = Info<U>;
+    struct Frame : Info<Constant> {
+        template<typename... Args>
+        Frame(Args &&... args) :
+            Info<Constant>(std::forward<Args>(args)...),
+            cur(this->val->use_empty() ? nullptr : &*this->val->use_begin()),
+            _next(cur ? cur->getNext() : nullptr)
+        {
+        }
+    private:
+        void next()
+        {
+            cur = _next;
+            if (!cur)
+                return;
+            _next = cur->getNext();
+        }
+        Use *cur;
+        Use *_next;
+        friend struct ConstantUses;
+    };
+    ConstantUses(Constant *c, Module &M)
+        : stack{Frame(nullptr, c, 0u, true)},
+          M(M)
+    {
+        forward();
+    }
+    UseInfo get_info() const
+    {
+        auto &top = stack.back();
+        return UseInfo(top.cur, top.offset, top.samebits);
+    }
+    const SmallVector<Frame, 4> &get_stack() const
+    {
+        return stack;
+    }
+    void next()
+    {
+        stack.back().next();
+        forward();
+    }
+    bool done()
+    {
+        return stack.empty();
+    }
+private:
+    void forward();
+    SmallVector<Frame, 4> stack;
+    Module &M;
+};
+
+template<typename U>
+void ConstantUses<U>::forward()
+{
+    assert(!stack.empty());
+    auto frame = &stack.back();
+    const DataLayout &DL = M.getDataLayout();
+    auto pop = [&] {
+        stack.pop_back();
+        if (stack.empty()) {
+            return false;
+        }
+        frame = &stack.back();
+        return true;
+    };
+    auto push = [&] (Use *use, Constant *c, size_t offset, bool samebits) {
+        stack.emplace_back(use, c, offset, samebits);
+        frame = &stack.back();
+    };
+    auto handle_constaggr = [&] (Use *use, ConstantAggregate *aggr) {
+        if (!frame->samebits) {
+            push(use, aggr, 0, false);
+            return;
+        }
+        if (auto strct = dyn_cast<ConstantStruct>(aggr)) {
+            auto layout = DL.getStructLayout(strct->getType());
+            push(use, strct, frame->offset + layout->getElementOffset(use->getOperandNo()), true);
+        }
+        else if (auto ary = dyn_cast<ConstantArray>(aggr)) {
+            auto elty = ary->getType()->getElementType();
+            push(use, ary, frame->offset + DL.getTypeAllocSize(elty) * use->getOperandNo(), true);
+        }
+        else if (auto vec = dyn_cast<ConstantVector>(aggr)) {
+            auto elty = vec->getType()->getElementType();
+            push(use, vec, frame->offset + DL.getTypeAllocSize(elty) * use->getOperandNo(), true);
+        }
+        else {
+            jl_safe_printf("Unknown ConstantAggregate:\n");
+            llvm_dump(aggr);
+            abort();
+        }
+    };
+    auto handle_constexpr = [&] (Use *use, ConstantExpr *expr) {
+        if (!frame->samebits) {
+            push(use, expr, 0, false);
+            return;
+        }
+        auto opcode = expr->getOpcode();
+        if (opcode == Instruction::PtrToInt || opcode == Instruction::IntToPtr ||
+            opcode == Instruction::AddrSpaceCast || opcode == Instruction::BitCast) {
+            push(use, expr, frame->offset, true);
+        }
+        else {
+            push(use, expr, 0, false);
+        }
+    };
+    while (true) {
+        auto use = frame->cur;
+        if (!use) {
+            if (!pop())
+                return;
+            continue;
+        }
+        auto user = use->getUser();
+        if (isa<U>(user))
+            return;
+        frame->next();
+        if (auto aggr = dyn_cast<ConstantAggregate>(user)) {
+            handle_constaggr(use, aggr);
+        }
+        else if (auto expr = dyn_cast<ConstantExpr>(user)) {
+            handle_constexpr(use, expr);
+        }
+    }
+}
+
+struct CloneCtx {
+    struct Target {
+        int idx;
+        uint32_t flags;
+        std::unique_ptr<ValueToValueMapTy> vmap; // ValueToValueMapTy is not movable....
+        // function ids that needs relocation to be initialized
+        std::set<uint32_t> relocs{};
+        Target(int idx, const jl_target_spec_t &spec) :
+            idx(idx),
+            flags(spec.flags),
+            vmap(new ValueToValueMapTy)
+        {
+        }
+    };
+    struct Group : Target {
+        std::vector<Target> clones;
+        std::set<uint32_t> clone_fs;
+        Group(int base, const jl_target_spec_t &spec) :
+            Target(base, spec),
+            clones{},
+            clone_fs{}
+        {}
+        Function *base_func(Function *orig_f) const
+        {
+            if (idx == 0)
+                return orig_f;
+            return cast<Function>(vmap->lookup(orig_f));
+        }
+    };
+    CloneCtx(MultiVersioning *pass, Module &M);
+    void clone_bases();
+    void collect_func_infos();
+    void clone_all_partials();
+    void fix_gv_uses();
+    void fix_inst_uses();
+    void emit_metadata();
+private:
+    void prepare_vmap(ValueToValueMapTy &vmap);
+    bool is_vector(FunctionType *ty) const;
+    void clone_function(Function *F, Function *new_f, ValueToValueMapTy &vmap);
+    uint32_t collect_func_info(Function &F);
+    void check_partial(Group &grp, Target &tgt);
+    void clone_partial(Group &grp, Target &tgt);
+    void add_features(Function *F, StringRef name, StringRef features, uint32_t flags) const;
+    template<typename T>
+    T *add_comdat(T *G) const;
+    uint32_t get_func_id(Function *F);
+    template<typename Stack>
+    Constant *rewrite_gv_init(const Stack& stack);
+    template<typename Stack>
+    Value *rewrite_inst_use(const Stack& stack, Value *replace, Instruction *insert_before);
+    std::pair<uint32_t,GlobalVariable*> get_reloc_slot(Function *F);
+    Constant *get_ptrdiff32(Constant *ptr, Constant *base) const;
+    template<typename T>
+    Constant *emit_offset_table(const std::vector<T*> &vars, StringRef name) const;
+    void rewrite_alias(GlobalAlias *alias, Function* F);
+
+    LLVMContext &ctx;
+    Type *T_size;
+    Type *T_int32;
+    Type *T_void;
+    PointerType *T_psize;
+    MDNode *tbaa_const;
+    MultiVersioning *pass;
+    std::vector<jl_target_spec_t> specs;
+    std::vector<Group> groups{};
+    std::vector<Function*> fvars;
+    std::vector<Constant*> gvars;
+    Module &M;
+    // Map from original functiton to one based index in `fvars`
+    std::map<const Function*,uint32_t> func_ids{};
+    std::vector<Function*> orig_funcs{};
+    std::vector<uint32_t> func_infos{};
+    std::set<Function*> cloned{};
+    // GV addresses and their corresponding function id (i.e. 0-based index in `fvars`)
+    std::vector<std::pair<Constant*,uint32_t>> gv_relocs{};
+    // Mapping from function id (i.e. 0-based index in `fvars`) to GVs to be initialized.
+    std::map<uint32_t,GlobalVariable*> const_relocs;
+    // Functions that were referred to by a global alias, and might not have other uses.
+    std::set<uint32_t> alias_relocs;
+    bool has_veccall{false};
+    bool has_cloneall{false};
+};
+
+struct MultiVersioning: public ModulePass {
+    static char ID;
+    MultiVersioning()
+        : ModulePass(ID)
+    {}
+
+private:
+    bool runOnModule(Module &M) override;
+    void getAnalysisUsage(AnalysisUsage &AU) const override
+    {
+        AU.addRequired<LoopInfoWrapperPass>();
+        AU.addRequired<CallGraphWrapperPass>();
+        AU.addPreserved<LoopInfoWrapperPass>();
+    }
+    friend struct CloneCtx;
+};
+
+template<typename T>
+static inline std::vector<T*> consume_gv(Module &M, const char *name)
+{
+    // Get information about sysimg export functions from the two global variables.
+    // Strip them from the Module so that it's easier to handle the uses.
+    GlobalVariable *gv = M.getGlobalVariable(name);
+    assert(gv && gv->hasInitializer());
+    auto *ary = cast<ConstantArray>(gv->getInitializer());
+    unsigned nele = ary->getNumOperands();
+    std::vector<T*> res(nele);
+    for (unsigned i = 0; i < nele; i++)
+        res[i] = cast<T>(ary->getOperand(i)->stripPointerCasts());
+    assert(gv->use_empty());
+    gv->eraseFromParent();
+    if (ary->use_empty())
+        ary->destroyConstant();
+    return res;
+}
+
+// Collect basic information about targets and functions.
+CloneCtx::CloneCtx(MultiVersioning *pass, Module &M)
+    : ctx(M.getContext()),
+      T_size(M.getDataLayout().getIntPtrType(ctx, 0)),
+      T_int32(Type::getInt32Ty(ctx)),
+      T_void(Type::getVoidTy(ctx)),
+      T_psize(PointerType::get(T_size, 0)),
+      tbaa_const(tbaa_make_child("jtbaa_const", nullptr, true).first),
+      pass(pass),
+      specs(jl_get_llvm_clone_targets()),
+      fvars(consume_gv<Function>(M, "jl_sysimg_fvars")),
+      gvars(consume_gv<Constant>(M, "jl_sysimg_gvars")),
+      M(M)
+{
+    groups.emplace_back(0, specs[0]);
+    uint32_t ntargets = specs.size();
+    for (uint32_t i = 1; i < ntargets; i++) {
+        auto &spec = specs[i];
+        if (spec.flags & JL_TARGET_CLONE_ALL) {
+            has_cloneall = true;
+            groups.emplace_back(i, spec);
+        }
+        else {
+            auto base = spec.base;
+            bool found = false;
+            for (auto &grp: groups) {
+                if (grp.idx == base) {
+                    found = true;
+                    grp.clones.emplace_back(i, spec);
+                    break;
+                }
+            }
+            (void)found;
+        }
+    }
+    uint32_t nfvars = fvars.size();
+    for (uint32_t i = 0; i < nfvars; i++)
+        func_ids[fvars[i]] = i + 1;
+    for (auto &F: M) {
+        if (F.empty())
+            continue;
+        orig_funcs.push_back(&F);
+    }
+}
+
+void CloneCtx::prepare_vmap(ValueToValueMapTy &vmap)
+{
+    // Workaround LLVM `CloneFunctionInfo` bug (?) pre-5.0
+    // The `DICompileUnit`s are being cloned but are not added to the `llvm.dbg.cu` metadata
+    // which triggers assertions when generating native code/in the verifier.
+    // Fix this by forcing an identical mapping for all `DICompileUnit` recorded.
+    // The `DISubprogram` cloning on LLVM 5.0 handles this
+    // but it doesn't hurt to enforce the identity either.
+    auto &MD = vmap.MD();
+    for (auto cu: M.debug_compile_units()) {
+        MD[cu].reset(cu);
+    }
+}
+
+void CloneCtx::clone_function(Function *F, Function *new_f, ValueToValueMapTy &vmap)
+{
+    Function::arg_iterator DestI = new_f->arg_begin();
+    for (Function::const_arg_iterator J = F->arg_begin(); J != F->arg_end(); ++J) {
+        DestI->setName(J->getName());
+        vmap[&*J] = &*DestI++;
+    }
+    SmallVector<ReturnInst*,8> Returns;
+#if JL_LLVM_VERSION >= 130000
+    // We are cloning into the same module
+    CloneFunctionInto(new_f, F, vmap, CloneFunctionChangeType::GlobalChanges, Returns);
+#else
+    CloneFunctionInto(new_f, F, vmap, true, Returns);
+#endif
+}
+
+// Clone all clone_all targets. Makes sure that the base targets are all available.
+void CloneCtx::clone_bases()
+{
+    if (!has_cloneall)
+        return;
+    uint32_t ngrps = groups.size();
+    for (uint32_t gid = 1; gid < ngrps; gid++) {
+        auto &grp = groups[gid];
+        auto suffix = ".clone_" + std::to_string(grp.idx);
+        auto &vmap = *grp.vmap;
+        // Fill in old->new mapping. We need to do this before cloning the function so that
+        // the intra target calls are automatically fixed up on cloning.
+        for (auto F: orig_funcs) {
+            Function *new_f = Function::Create(F->getFunctionType(), F->getLinkage(),
+                                               F->getName() + suffix, &M);
+            new_f->copyAttributesFrom(F);
+            vmap[F] = new_f;
+        }
+        prepare_vmap(vmap);
+        for (auto F: orig_funcs) {
+            clone_function(F, cast<Function>(vmap.lookup(F)), vmap);
+        }
+    }
+}
+
+bool CloneCtx::is_vector(FunctionType *ty) const
+{
+    if (ty->getReturnType()->isVectorTy())
+        return true;
+    for (auto arg: ty->params()) {
+        if (arg->isVectorTy()) {
+            return true;
+        }
+    }
+    return false;
+}
+
+uint32_t CloneCtx::collect_func_info(Function &F)
+{
+    uint32_t flag = 0;
+    if (!pass->getAnalysis<LoopInfoWrapperPass>(F).getLoopInfo().empty())
+        flag |= JL_TARGET_CLONE_LOOP;
+    if (is_vector(F.getFunctionType())) {
+        flag |= JL_TARGET_CLONE_SIMD;
+        has_veccall = true;
+    }
+    for (auto &bb: F) {
+        for (auto &I: bb) {
+            if (auto call = dyn_cast<CallInst>(&I)) {
+                if (is_vector(call->getFunctionType())) {
+                    has_veccall = true;
+                    flag |= JL_TARGET_CLONE_SIMD;
+                }
+                if (auto callee = call->getCalledFunction()) {
+                    auto name = callee->getName();
+                    if (name.startswith("llvm.muladd.") || name.startswith("llvm.fma.")) {
+                        flag |= JL_TARGET_CLONE_MATH;
+                    }
+                }
+            }
+            else if (auto store = dyn_cast<StoreInst>(&I)) {
+                if (store->getValueOperand()->getType()->isVectorTy()) {
+                    flag |= JL_TARGET_CLONE_SIMD;
+                }
+            }
+            else if (I.getType()->isVectorTy()) {
+                flag |= JL_TARGET_CLONE_SIMD;
+            }
+            if (auto mathOp = dyn_cast<FPMathOperator>(&I)) {
+                if (mathOp->getFastMathFlags().any()) {
+                    flag |= JL_TARGET_CLONE_MATH;
+                }
+            }
+            if (has_veccall && (flag & JL_TARGET_CLONE_SIMD) && (flag & JL_TARGET_CLONE_MATH)) {
+                return flag;
+            }
+        }
+    }
+    return flag;
+}
+
+void CloneCtx::collect_func_infos()
+{
+    uint32_t nfuncs = orig_funcs.size();
+    func_infos.resize(nfuncs);
+    for (uint32_t i = 0; i < nfuncs; i++) {
+        func_infos[i] = collect_func_info(*orig_funcs[i]);
+    }
+}
+
+void CloneCtx::clone_all_partials()
+{
+    // First decide what to clone
+    // Do this before actually cloning the functions
+    // so that the call graph is easier to understand
+    for (auto &grp: groups) {
+        for (auto &tgt: grp.clones) {
+            check_partial(grp, tgt);
+        }
+    }
+    for (auto &grp: groups) {
+        for (auto &tgt: grp.clones)
+            clone_partial(grp, tgt);
+        // Also set feature strings for base target functions
+        // now that all the actual cloning is done.
+        auto &base_spec = specs[grp.idx];
+        for (auto orig_f: orig_funcs) {
+            add_features(grp.base_func(orig_f), base_spec.cpu_name,
+                         base_spec.cpu_features, base_spec.flags);
+        }
+    }
+    func_infos.clear(); // We don't need this anymore
+}
+
+void CloneCtx::check_partial(Group &grp, Target &tgt)
+{
+    auto flag = specs[tgt.idx].flags & clone_mask;
+    auto suffix = ".clone_" + std::to_string(tgt.idx);
+    auto &vmap = *tgt.vmap;
+    uint32_t nfuncs = func_infos.size();
+
+    std::set<Function*> all_origs;
+    // Use a simple heuristic to decide which function we need to clone.
+    for (uint32_t i = 0; i < nfuncs; i++) {
+        if (!(func_infos[i] & flag))
+            continue;
+        auto orig_f = orig_funcs[i];
+        // Fill in old->new mapping. We need to do this before cloning the function so that
+        // the intra target calls are automatically fixed up on cloning.
+        auto F = grp.base_func(orig_f);
+        Function *new_f = Function::Create(F->getFunctionType(), F->getLinkage(),
+                                           F->getName() + suffix, &M);
+        new_f->copyAttributesFrom(F);
+        vmap[F] = new_f;
+        if (!has_cloneall)
+            cloned.insert(orig_f);
+        grp.clone_fs.insert(i);
+        all_origs.insert(orig_f);
+    }
+    std::set<Function*> sets[2]{all_origs, std::set<Function*>{}};
+    auto *cur_set = &sets[0];
+    auto *next_set = &sets[1];
+    // Reduce dispatch by expand the cloning set to functions that are directly called by
+    // and calling cloned functions.
+    auto &graph = pass->getAnalysis<CallGraphWrapperPass>().getCallGraph();
+    while (!cur_set->empty()) {
+        for (auto orig_f: *cur_set) {
+            // Use the uncloned function since it's already in the call graph
+            auto node = graph[orig_f];
+            for (const auto &I: *node) {
+                auto child_node = I.second;
+                auto orig_child_f = child_node->getFunction();
+                if (!orig_child_f)
+                    continue;
+                // Already cloned
+                if (all_origs.count(orig_child_f))
+                    continue;
+                bool calling_clone = false;
+                for (const auto &I2: *child_node) {
+                    auto orig_child_f2 = I2.second->getFunction();
+                    if (!orig_child_f2)
+                        continue;
+                    if (all_origs.count(orig_child_f2)) {
+                        calling_clone = true;
+                        break;
+                    }
+                }
+                if (!calling_clone)
+                    continue;
+                next_set->insert(orig_child_f);
+                all_origs.insert(orig_child_f);
+                auto child_f = grp.base_func(orig_child_f);
+                Function *new_f = Function::Create(child_f->getFunctionType(),
+                                                   child_f->getLinkage(),
+                                                   child_f->getName() + suffix, &M);
+                new_f->copyAttributesFrom(child_f);
+                vmap[child_f] = new_f;
+            }
+        }
+        std::swap(cur_set, next_set);
+        next_set->clear();
+    }
+    for (uint32_t i = 0; i < nfuncs; i++) {
+        // Only need to handle expanded functions
+        if (func_infos[i] & flag)
+            continue;
+        auto orig_f = orig_funcs[i];
+        if (all_origs.count(orig_f)) {
+            if (!has_cloneall)
+                cloned.insert(orig_f);
+            grp.clone_fs.insert(i);
+        }
+    }
+}
+
+void CloneCtx::clone_partial(Group &grp, Target &tgt)
+{
+    auto &spec = specs[tgt.idx];
+    auto &vmap = *tgt.vmap;
+    uint32_t nfuncs = orig_funcs.size();
+    prepare_vmap(vmap);
+    for (uint32_t i = 0; i < nfuncs; i++) {
+        auto orig_f = orig_funcs[i];
+        auto F = grp.base_func(orig_f);
+        if (auto new_v = map_get(vmap, F)) {
+            auto new_f = cast<Function>(new_v);
+            assert(new_f != F);
+            clone_function(F, new_f, vmap);
+            // We can set the feature strings now since no one is going to
+            // clone these functions again.
+            add_features(new_f, spec.cpu_name, spec.cpu_features, spec.flags);
+        }
+    }
+}
+
+void CloneCtx::add_features(Function *F, StringRef name, StringRef features, uint32_t flags) const
+{
+    auto attr = F->getFnAttribute("target-features");
+    if (attr.isStringAttribute()) {
+        std::string new_features(attr.getValueAsString());
+        new_features += ",";
+        new_features += features;
+        F->addFnAttr("target-features", new_features);
+    }
+    else {
+        F->addFnAttr("target-features", features);
+    }
+    F->addFnAttr("target-cpu", name);
+    if (!F->hasFnAttribute(Attribute::OptimizeNone)) {
+        if (flags & JL_TARGET_OPTSIZE) {
+            F->addFnAttr(Attribute::OptimizeForSize);
+        }
+        else if (flags & JL_TARGET_MINSIZE) {
+            F->addFnAttr(Attribute::MinSize);
+        }
+    }
+}
+
+uint32_t CloneCtx::get_func_id(Function *F)
+{
+    auto &ref = func_ids[F];
+    if (!ref) {
+        fvars.push_back(F);
+        ref = fvars.size();
+    }
+    return ref - 1;
+}
+
+template<typename Stack>
+Constant *CloneCtx::rewrite_gv_init(const Stack& stack)
+{
+    // Null initialize so that LLVM put it in the correct section.
+    SmallVector<Constant*, 8> args;
+    Constant *res = ConstantPointerNull::get(cast<PointerType>(stack[0].val->getType()));
+    uint32_t nlevel = stack.size();
+    for (uint32_t i = 1; i < nlevel; i++) {
+        auto &frame = stack[i];
+        auto val = frame.val;
+        Use *use = frame.use;
+        unsigned idx = use->getOperandNo();
+        unsigned nargs = val->getNumOperands();
+        args.resize(nargs);
+        for (unsigned j = 0; j < nargs; j++) {
+            if (idx == j) {
+                args[j] = res;
+            }
+            else {
+                args[j] = cast<Constant>(val->getOperand(j));
+            }
+        }
+        if (auto expr = dyn_cast<ConstantExpr>(val)) {
+            res = expr->getWithOperands(args);
+        }
+        else if (auto ary = dyn_cast<ConstantArray>(val)) {
+            res = ConstantArray::get(ary->getType(), args);
+        }
+        else if (auto strct = dyn_cast<ConstantStruct>(val)) {
+            res = ConstantStruct::get(strct->getType(), args);
+        }
+        else if (isa<ConstantVector>(val)) {
+            res = ConstantVector::get(args);
+        }
+        else {
+            jl_safe_printf("Unknown const use.");
+            llvm_dump(val);
+            abort();
+        }
+    }
+    return res;
+}
+
+// replace an alias to a function with a trampoline and (uninitialized) global variable slot
+void CloneCtx::rewrite_alias(GlobalAlias *alias, Function *F)
+{
+    assert(!is_vector(F->getFunctionType()));
+
+    Function *trampoline =
+        Function::Create(F->getFunctionType(), alias->getLinkage(), "", &M);
+    trampoline->copyAttributesFrom(F);
+    trampoline->takeName(alias);
+    alias->eraseFromParent();
+
+    uint32_t id;
+    GlobalVariable *slot;
+    std::tie(id, slot) = get_reloc_slot(F);
+    for (auto &grp: groups) {
+        grp.relocs.insert(id);
+        for (auto &tgt: grp.clones) {
+            tgt.relocs.insert(id);
+        }
+    }
+    alias_relocs.insert(id);
+
+    auto BB = BasicBlock::Create(ctx, "top", trampoline);
+    IRBuilder<> irbuilder(BB);
+
+    auto ptr = irbuilder.CreateLoad(F->getType(), slot);
+    ptr->setMetadata(llvm::LLVMContext::MD_tbaa, tbaa_const);
+    ptr->setMetadata(llvm::LLVMContext::MD_invariant_load, MDNode::get(ctx, None));
+
+    std::vector<Value *> Args;
+    for (auto &arg : trampoline->args())
+        Args.push_back(&arg);
+    auto call = irbuilder.CreateCall(F->getFunctionType(), ptr, makeArrayRef(Args));
+    if (F->isVarArg())
+#if (defined(_CPU_ARM_) || defined(_CPU_PPC_) || defined(_CPU_PPC64_))
+        abort();    // musttail support is very bad on ARM, PPC, PPC64 (as of LLVM 3.9)
+#else
+        call->setTailCallKind(CallInst::TCK_MustTail);
+#endif
+    else
+        call->setTailCallKind(CallInst::TCK_Tail);
+
+    if (F->getReturnType() == T_void)
+        irbuilder.CreateRetVoid();
+    else
+        irbuilder.CreateRet(call);
+}
+
+void CloneCtx::fix_gv_uses()
+{
+    auto single_pass = [&] (Function *orig_f) {
+        bool changed = false;
+        for (auto uses = ConstantUses<GlobalValue>(orig_f, M); !uses.done(); uses.next()) {
+            changed = true;
+            auto &stack = uses.get_stack();
+            auto info = uses.get_info();
+            // We only support absolute pointer relocation.
+            assert(info.samebits);
+            GlobalVariable *val;
+            if (auto alias = dyn_cast<GlobalAlias>(info.val)) {
+                rewrite_alias(alias, orig_f);
+                continue;
+            }
+            else {
+                val = cast<GlobalVariable>(info.val);
+            }
+            assert(info.use->getOperandNo() == 0);
+            assert(!val->isConstant());
+            auto fid = get_func_id(orig_f);
+            auto addr = ConstantExpr::getPtrToInt(val, T_size);
+            if (info.offset)
+                addr = ConstantExpr::getAdd(addr, ConstantInt::get(T_size, info.offset));
+            gv_relocs.emplace_back(addr, fid);
+            val->setInitializer(rewrite_gv_init(stack));
+        }
+        return changed;
+    };
+    for (auto orig_f: orig_funcs) {
+        if (!has_cloneall && !cloned.count(orig_f))
+            continue;
+        while (single_pass(orig_f)) {
+        }
+    }
+}
+
+std::pair<uint32_t,GlobalVariable*> CloneCtx::get_reloc_slot(Function *F)
+{
+    // Null initialize so that LLVM put it in the correct section.
+    auto id = get_func_id(F);
+    auto &slot = const_relocs[id];
+    if (!slot)
+        slot = new GlobalVariable(M, F->getType(), false, GlobalVariable::InternalLinkage,
+                                  ConstantPointerNull::get(F->getType()),
+                                  F->getName() + ".reloc_slot");
+    return std::make_pair(id, slot);
+}
+
+template<typename Stack>
+Value *CloneCtx::rewrite_inst_use(const Stack& stack, Value *replace, Instruction *insert_before)
+{
+    SmallVector<Constant*, 8> args;
+    uint32_t nlevel = stack.size();
+    for (uint32_t i = 1; i < nlevel; i++) {
+        auto &frame = stack[i];
+        auto val = frame.val;
+        Use *use = frame.use;
+        unsigned idx = use->getOperandNo();
+        if (auto expr = dyn_cast<ConstantExpr>(val)) {
+            auto inst = expr->getAsInstruction();
+            inst->replaceUsesOfWith(val->getOperand(idx), replace);
+            inst->insertBefore(insert_before);
+            replace = inst;
+            continue;
+        }
+        unsigned nargs = val->getNumOperands();
+        args.resize(nargs);
+        for (unsigned j = 0; j < nargs; j++) {
+            auto op = val->getOperand(j);
+            if (idx == j) {
+                args[j] = UndefValue::get(op->getType());
+            }
+            else {
+                args[j] = cast<Constant>(op);
+            }
+        }
+        if (auto ary = dyn_cast<ConstantArray>(val)) {
+            replace = InsertValueInst::Create(ConstantArray::get(ary->getType(), args),
+                                              replace, {idx}, "", insert_before);
+        }
+        else if (auto strct = dyn_cast<ConstantStruct>(val)) {
+            replace = InsertValueInst::Create(ConstantStruct::get(strct->getType(), args),
+                                              replace, {idx}, "", insert_before);
+        }
+        else if (isa<ConstantVector>(val)) {
+            replace = InsertElementInst::Create(ConstantVector::get(args), replace,
+                                                ConstantInt::get(T_size, idx), "",
+                                                insert_before);
+        }
+        else {
+            jl_safe_printf("Unknown const use.");
+            llvm_dump(val);
+            abort();
+        }
+    }
+    return replace;
+}
+
+void CloneCtx::fix_inst_uses()
+{
+    uint32_t nfuncs = orig_funcs.size();
+    for (auto &grp: groups) {
+        auto suffix = ".clone_" + std::to_string(grp.idx);
+        for (uint32_t i = 0; i < nfuncs; i++) {
+            if (!grp.clone_fs.count(i))
+                continue;
+            auto orig_f = orig_funcs[i];
+            auto F = grp.base_func(orig_f);
+            bool changed;
+            do {
+                changed = false;
+                for (auto uses = ConstantUses<Instruction>(F, M); !uses.done(); uses.next()) {
+                    auto info = uses.get_info();
+                    auto use_i = info.val;
+                    auto use_f = use_i->getFunction();
+                    if (!use_f->getName().endswith(suffix))
+                        continue;
+                    Instruction *insert_before = use_i;
+                    if (auto phi = dyn_cast<PHINode>(use_i))
+                        insert_before = phi->getIncomingBlock(*info.use)->getTerminator();
+                    uint32_t id;
+                    GlobalVariable *slot;
+                    std::tie(id, slot) = get_reloc_slot(orig_f);
+                    Instruction *ptr = new LoadInst(orig_f->getType(), slot, "", false, insert_before);
+                    ptr->setMetadata(llvm::LLVMContext::MD_tbaa, tbaa_const);
+                    ptr->setMetadata(llvm::LLVMContext::MD_invariant_load, MDNode::get(ctx, None));
+                    use_i->setOperand(info.use->getOperandNo(),
+                                      rewrite_inst_use(uses.get_stack(), ptr,
+                                                       insert_before));
+
+                    grp.relocs.insert(id);
+                    for (auto &tgt: grp.clones) {
+                        // The enclosing function of the use is cloned,
+                        // no need to deal with this use on this target.
+                        if (map_get(*tgt.vmap, use_f))
+                            continue;
+                        tgt.relocs.insert(id);
+                    }
+
+                    changed = true;
+                }
+            } while (changed);
+        }
+    }
+}
+
+template<typename T>
+inline T *CloneCtx::add_comdat(T *G) const
+{
+#if defined(_OS_WINDOWS_)
+    // Add comdat information to make MSVC link.exe happy
+    // it's valid to emit this for ld.exe too,
+    // but makes it very slow to link for no benefit
+#if defined(_COMPILER_MICROSOFT_)
+    Comdat *jl_Comdat = G->getParent()->getOrInsertComdat(G->getName());
+    // ELF only supports Comdat::Any
+    jl_Comdat->setSelectionKind(Comdat::NoDuplicates);
+    G->setComdat(jl_Comdat);
+#endif
+    // add __declspec(dllexport) to everything marked for export
+    if (G->getLinkage() == GlobalValue::ExternalLinkage)
+        G->setDLLStorageClass(GlobalValue::DLLExportStorageClass);
+    else
+        G->setDLLStorageClass(GlobalValue::DefaultStorageClass);
+#endif
+    return G;
+}
+
+Constant *CloneCtx::get_ptrdiff32(Constant *ptr, Constant *base) const
+{
+    if (ptr->getType()->isPointerTy())
+        ptr = ConstantExpr::getPtrToInt(ptr, T_size);
+    auto ptrdiff = ConstantExpr::getSub(ptr, base);
+    return sizeof(void*) == 8 ? ConstantExpr::getTrunc(ptrdiff, T_int32) : ptrdiff;
+}
+
+template<typename T>
+Constant *CloneCtx::emit_offset_table(const std::vector<T*> &vars, StringRef name) const
+{
+    assert(!vars.empty());
+    add_comdat(GlobalAlias::create(T_size, 0, GlobalVariable::ExternalLinkage,
+                                   name + "_base",
+                                   ConstantExpr::getBitCast(vars[0], T_psize), &M));
+    auto vbase = ConstantExpr::getPtrToInt(vars[0], T_size);
+    uint32_t nvars = vars.size();
+    std::vector<Constant*> offsets(nvars + 1);
+    offsets[0] = ConstantInt::get(T_int32, nvars);
+    offsets[1] = ConstantInt::get(T_int32, 0);
+    for (uint32_t i = 1; i < nvars; i++)
+        offsets[i + 1] = get_ptrdiff32(vars[i], vbase);
+    ArrayType *vars_type = ArrayType::get(T_int32, nvars + 1);
+    add_comdat(new GlobalVariable(M, vars_type, true,
+                                  GlobalVariable::ExternalLinkage,
+                                  ConstantArray::get(vars_type, offsets),
+                                  name + "_offsets"));
+    return vbase;
+}
+
+void CloneCtx::emit_metadata()
+{
+    // Store back the information about exported functions.
+    auto fbase = emit_offset_table(fvars, "jl_sysimg_fvars");
+    auto gbase = emit_offset_table(gvars, "jl_sysimg_gvars");
+    uint32_t nfvars = fvars.size();
+
+    uint32_t ntargets = specs.size();
+    SmallVector<Target*, 8> targets(ntargets);
+    for (auto &grp: groups) {
+        targets[grp.idx] = &grp;
+        for (auto &tgt: grp.clones) {
+            targets[tgt.idx] = &tgt;
+        }
+    }
+
+    // Generate `jl_dispatch_target_ids`
+    {
+        const uint32_t base_flags = has_veccall ? JL_TARGET_VEC_CALL : 0;
+        std::vector<uint8_t> data;
+        auto push_i32 = [&] (uint32_t v) {
+            uint8_t buff[4];
+            memcpy(buff, &v, 4);
+            data.insert(data.end(), buff, buff + 4);
+        };
+        push_i32(ntargets);
+        for (uint32_t i = 0; i < ntargets; i++) {
+            push_i32(base_flags | (specs[i].flags & JL_TARGET_UNKNOWN_NAME));
+            auto &specdata = specs[i].data;
+            data.insert(data.end(), specdata.begin(), specdata.end());
+        }
+        auto value = ConstantDataArray::get(ctx, data);
+        add_comdat(new GlobalVariable(M, value->getType(), true,
+                                      GlobalVariable::ExternalLinkage,
+                                      value, "jl_dispatch_target_ids"));
+    }
+
+    // Generate `jl_dispatch_reloc_slots`
+    std::set<uint32_t> shared_relocs;
+    {
+        std::stable_sort(gv_relocs.begin(), gv_relocs.end(),
+                         [] (const std::pair<Constant*,uint32_t> &lhs,
+                             const std::pair<Constant*,uint32_t> &rhs) {
+                             return lhs.second < rhs.second;
+                         });
+        std::vector<Constant*> values{nullptr};
+        uint32_t gv_reloc_idx = 0;
+        uint32_t ngv_relocs = gv_relocs.size();
+        for (uint32_t id = 0; id < nfvars; id++) {
+            // TODO:
+            // explicitly set section? so that we are sure the relocation slots
+            // are in the same section as `gbase`.
+            auto id_v = ConstantInt::get(T_int32, id);
+            for (; gv_reloc_idx < ngv_relocs && gv_relocs[gv_reloc_idx].second == id;
+                 gv_reloc_idx++) {
+                shared_relocs.insert(id);
+                values.push_back(id_v);
+                values.push_back(get_ptrdiff32(gv_relocs[gv_reloc_idx].first, gbase));
+            }
+            auto it = const_relocs.find(id);
+            if (it != const_relocs.end()) {
+                values.push_back(id_v);
+                values.push_back(get_ptrdiff32(it->second, gbase));
+            }
+            if (alias_relocs.find(id) != alias_relocs.end()) {
+                shared_relocs.insert(id);
+            }
+        }
+        values[0] = ConstantInt::get(T_int32, values.size() / 2);
+        ArrayType *vars_type = ArrayType::get(T_int32, values.size());
+        add_comdat(new GlobalVariable(M, vars_type, true, GlobalVariable::ExternalLinkage,
+                                      ConstantArray::get(vars_type, values),
+                                      "jl_dispatch_reloc_slots"));
+    }
+
+    // Generate `jl_dispatch_fvars_idxs` and `jl_dispatch_fvars_offsets`
+    {
+        std::vector<uint32_t> idxs;
+        std::vector<Constant*> offsets;
+        for (uint32_t i = 0; i < ntargets; i++) {
+            auto tgt = targets[i];
+            auto &spec = specs[i];
+            uint32_t len_idx = idxs.size();
+            idxs.push_back(0); // We will fill in the real value later.
+            uint32_t count = 0;
+            if (i == 0 || spec.flags & JL_TARGET_CLONE_ALL) {
+                auto grp = static_cast<Group*>(tgt);
+                count = jl_sysimg_tag_mask;
+                for (uint32_t j = 0; j < nfvars; j++) {
+                    if (shared_relocs.count(j) || tgt->relocs.count(j)) {
+                        count++;
+                        idxs.push_back(j);
+                    }
+                    if (i != 0) {
+                        offsets.push_back(get_ptrdiff32(grp->base_func(fvars[j]), fbase));
+                    }
+                }
+            }
+            else {
+                auto baseidx = spec.base;
+                auto grp = static_cast<Group*>(targets[baseidx]);
+                idxs.push_back(baseidx);
+                for (uint32_t j = 0; j < nfvars; j++) {
+                    auto base_f = grp->base_func(fvars[j]);
+                    if (shared_relocs.count(j) || tgt->relocs.count(j)) {
+                        count++;
+                        idxs.push_back(jl_sysimg_tag_mask | j);
+                        auto f = map_get(*tgt->vmap, base_f, base_f);
+                        offsets.push_back(get_ptrdiff32(cast<Function>(f), fbase));
+                    }
+                    else if (auto f = map_get(*tgt->vmap, base_f)) {
+                        count++;
+                        idxs.push_back(j);
+                        offsets.push_back(get_ptrdiff32(cast<Function>(f), fbase));
+                    }
+                }
+            }
+            idxs[len_idx] = count;
+        }
+        auto idxval = ConstantDataArray::get(ctx, idxs);
+        add_comdat(new GlobalVariable(M, idxval->getType(), true,
+                                      GlobalVariable::ExternalLinkage,
+                                      idxval, "jl_dispatch_fvars_idxs"));
+        ArrayType *offsets_type = ArrayType::get(T_int32, offsets.size());
+        add_comdat(new GlobalVariable(M, offsets_type, true,
+                                      GlobalVariable::ExternalLinkage,
+                                      ConstantArray::get(offsets_type, offsets),
+                                      "jl_dispatch_fvars_offsets"));
+    }
+}
+
+bool MultiVersioning::runOnModule(Module &M)
+{
+    // Group targets and identify cloning bases.
+    // Also initialize function info maps (we'll update these maps as we go)
+    // Maps that we need includes,
+    //
+    //     * Original function -> ID (initialize from `fvars` and allocate ID lazily)
+    //     * Cloned function -> Original function (add as we clone functions)
+    //     * Original function -> Base function (target specific and updated by LLVM)
+    //     * ID -> relocation slots (const).
+    if (M.getName() == "sysimage")
+        return false;
+
+    CloneCtx clone(this, M);
+
+    // Collect a list of original functions and clone base functions
+    clone.clone_bases();
+
+    // Collect function info (type of instruction used)
+    clone.collect_func_infos();
+
+    // If any partially cloned target exist decide which functions to clone for these targets.
+    // Clone functions for each group and collect a list of them.
+    // We can also add feature strings for cloned functions
+    // now that no additional cloning needs to be done.
+    clone.clone_all_partials();
+
+    // Scan **ALL** cloned functions (including full cloning for base target)
+    // for global variables initialization use.
+    // Replace them with `null` slot to be initialized at runtime and record relocation slot.
+    // These relocations must be initialized for **ALL** targets.
+    clone.fix_gv_uses();
+
+    // For each group, scan all functions cloned by **PARTIALLY** cloned targets for
+    // instruction use.
+    // A function needs a const relocation slot if it is cloned and is called by a
+    // uncloned function for at least one partially cloned target in the group.
+    // This is also the condition that a use in an uncloned function needs to be replaced with
+    // a slot load (i.e. if both the caller and the callee are always cloned or not cloned
+    // on all targets, the caller site does not need a relocation slot).
+    // A target needs a slot to be initialized iff at least one caller is not initialized.
+    clone.fix_inst_uses();
+
+    // Store back sysimg information with the correct format.
+    // At this point, we should have fixed up all the uses of the cloned functions
+    // and collected all the shared/target-specific relocations.
+    clone.emit_metadata();
+
+    return true;
+}
+
+char MultiVersioning::ID = 0;
+static RegisterPass<MultiVersioning> X("JuliaMultiVersioning", "JuliaMultiVersioning Pass",
+                                       false /* Only looks at CFG */,
+                                       false /* Analysis Pass */);
+
+}
+
+Pass *createMultiVersioningPass()
+{
+    return new MultiVersioning();
+}
+
+extern "C" JL_DLLEXPORT void LLVMExtraAddMultiVersioningPass(LLVMPassManagerRef PM)
+{
+    unwrap(PM)->add(createMultiVersioningPass());
+}
diff -Naur julia-1.7.3/src/llvm-pass-helpers.cpp julia-1.7.3.new/src/llvm-pass-helpers.cpp
--- julia-1.7.3/src/llvm-pass-helpers.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/llvm-pass-helpers.cpp	2022-07-16 20:42:45.798770900 +0200
@@ -18,27 +18,31 @@
 
 using namespace llvm;
 
-extern std::pair<MDNode*,MDNode*> tbaa_make_child(const char *name, MDNode *parent=nullptr, bool isConstant=false);
-
 JuliaPassContext::JuliaPassContext()
     : T_size(nullptr), T_int8(nullptr), T_int32(nullptr),
         T_pint8(nullptr), T_jlvalue(nullptr), T_prjlvalue(nullptr),
         T_ppjlvalue(nullptr), T_pjlvalue(nullptr), T_pjlvalue_der(nullptr),
-        T_ppjlvalue_der(nullptr), pgcstack_getter(nullptr), gc_flush_func(nullptr),
+        T_ppjlvalue_der(nullptr),
+
+        tbaa_gcframe(nullptr), tbaa_tag(nullptr),
+
+        pgcstack_getter(nullptr), gc_flush_func(nullptr),
         gc_preserve_begin_func(nullptr), gc_preserve_end_func(nullptr),
         pointer_from_objref_func(nullptr), alloc_obj_func(nullptr),
         typeof_func(nullptr), write_barrier_func(nullptr), module(nullptr)
 {
-    tbaa_gcframe = tbaa_make_child("jtbaa_gcframe").first;
-    MDNode *tbaa_data;
-    MDNode *tbaa_data_scalar;
-    std::tie(tbaa_data, tbaa_data_scalar) = tbaa_make_child("jtbaa_data");
-    tbaa_tag = tbaa_make_child("jtbaa_tag", tbaa_data_scalar).first;
 }
 
 void JuliaPassContext::initFunctions(Module &M)
 {
     module = &M;
+    LLVMContext &llvmctx = M.getContext();
+
+    tbaa_gcframe = tbaa_make_child_with_context(llvmctx, "jtbaa_gcframe").first;
+    MDNode *tbaa_data;
+    MDNode *tbaa_data_scalar;
+    std::tie(tbaa_data, tbaa_data_scalar) = tbaa_make_child_with_context(llvmctx, "jtbaa_data");
+    tbaa_tag = tbaa_make_child_with_context(llvmctx, "jtbaa_tag", tbaa_data_scalar).first;
 
     pgcstack_getter = M.getFunction("julia.get_pgcstack");
     gc_flush_func = M.getFunction("julia.gcroot_flush");
diff -Naur julia-1.7.3/src/llvm-ptls.cpp julia-1.7.3.new/src/llvm-ptls.cpp
--- julia-1.7.3/src/llvm-ptls.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/llvm-ptls.cpp	2022-07-16 20:42:45.798770900 +0200
@@ -32,9 +32,6 @@
 
 typedef Instruction TerminatorInst;
 
-std::pair<MDNode*,MDNode*> tbaa_make_child(const char *name, MDNode *parent=nullptr,
-                                           bool isConstant=false);
-
 namespace {
 
 struct LowerPTLS: public ModulePass {
@@ -273,7 +270,7 @@
         return false;
 
     ctx = &M->getContext();
-    tbaa_const = tbaa_make_child("jtbaa_const", nullptr, true).first;
+    tbaa_const = tbaa_make_child_with_context(*ctx, "jtbaa_const", nullptr, true).first;
 
     T_int8 = Type::getInt8Ty(*ctx);
     T_size = sizeof(size_t) == 8 ? Type::getInt64Ty(*ctx) : Type::getInt32Ty(*ctx);
diff -Naur julia-1.7.3/src/llvm-remove-addrspaces.cpp julia-1.7.3.new/src/llvm-remove-addrspaces.cpp
--- julia-1.7.3/src/llvm-remove-addrspaces.cpp	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/llvm-remove-addrspaces.cpp	2022-07-16 20:42:45.798770900 +0200
@@ -344,7 +344,11 @@
         for (auto MD : MDs)
             NGV->addMetadata(
                     MD.first,
+#if JL_LLVM_VERSION >= 130000
+                    *MapMetadata(MD.second, VMap));
+#else
                     *MapMetadata(MD.second, VMap, RF_MoveDistinctMDs));
+#endif
 
         copyComdat(NGV, GV);
 
@@ -371,7 +375,11 @@
                 NF,
                 F,
                 VMap,
+#if JL_LLVM_VERSION >= 130000
+                CloneFunctionChangeType::GlobalChanges,
+#else
                 /*ModuleLevelChanges=*/true,
+#endif
                 Returns,
                 "",
                 nullptr,
diff -Naur julia-1.7.3/src/Makefile julia-1.7.3.new/src/Makefile
--- julia-1.7.3/src/Makefile	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/src/Makefile	2022-07-16 20:42:45.798770900 +0200
@@ -247,7 +247,7 @@
 $(BUILDDIR)/llvm-alloc-opt.o $(BUILDDIR)/llvm-alloc-opt.dbg.obj: $(SRCDIR)/codegen_shared.h
 $(BUILDDIR)/llvm-final-gc-lowering.o $(BUILDDIR)/llvm-final-gc-lowering.dbg.obj: $(SRCDIR)/llvm-pass-helpers.h
 $(BUILDDIR)/llvm-gc-invariant-verifier.o $(BUILDDIR)/llvm-gc-invariant-verifier.dbg.obj: $(SRCDIR)/codegen_shared.h
-$(BUILDDIR)/llvm-late-gc-lowering.o $(BUILDDIR)/llvm-late-gc-lowering.dbg.obj: $(SRCDIR)/llvm-pass-helpers.h
+$(BUILDDIR)/llvm-late-gc-lowering.o $(BUILDDIR)/llvm-late-gc-lowering.dbg.obj: $(SRCDIR)/llvm-pass-helpers.h $(SRCDIR)/codegen_shared.h
 $(BUILDDIR)/llvm-multiversioning.o $(BUILDDIR)/llvm-multiversioning.dbg.obj: $(SRCDIR)/codegen_shared.h
 $(BUILDDIR)/llvm-pass-helpers.o $(BUILDDIR)/llvm-pass-helpers.dbg.obj: $(SRCDIR)/llvm-pass-helpers.h $(SRCDIR)/codegen_shared.h
 $(BUILDDIR)/llvm-ptls.o $(BUILDDIR)/llvm-ptls.dbg.obj: $(SRCDIR)/codegen_shared.h
diff -Naur julia-1.7.3/test/llvmpasses/late-lower-gc.ll julia-1.7.3.new/test/llvmpasses/late-lower-gc.ll
--- julia-1.7.3/test/llvmpasses/late-lower-gc.ll	2022-05-18 07:08:54.000000000 +0200
+++ julia-1.7.3.new/test/llvmpasses/late-lower-gc.ll	2022-07-16 20:42:45.798770900 +0200
@@ -96,6 +96,25 @@
 ; CHECK: ret i32
 }
 
+define i32 @freeze({} addrspace(10)* %v0, {} addrspace(10)* %v1) {
+top:
+; CHECK-LABEL: @freeze
+; CHECK-NOT: @julia.new_gc_frame
+  %v2 = call {}*** @julia.get_pgcstack()
+  %v3 = bitcast {} addrspace(10)* %v0 to {} addrspace(10)* addrspace(10)*
+  %v4 = addrspacecast {} addrspace(10)* addrspace(10)* %v3 to {} addrspace(10)* addrspace(11)*
+  %v5 = load atomic {} addrspace(10)*, {} addrspace(10)* addrspace(11)* %v4 unordered, align 8
+  %v6 = bitcast {} addrspace(10)* %v1 to {} addrspace(10)* addrspace(10)*
+  %v7 = addrspacecast {} addrspace(10)* addrspace(10)* %v6 to {} addrspace(10)* addrspace(11)*
+  %v8 = load atomic {} addrspace(10)*, {} addrspace(10)* addrspace(11)* %v7 unordered, align 8
+  %fv8 = freeze {} addrspace(10)* %v8
+  %v9 = addrspacecast {} addrspace(10)* %v5 to {} addrspace(12)*
+  %v10 = addrspacecast {} addrspace(10)* %fv8 to {} addrspace(12)*
+  %v11 = call i32 @rooting_callee({} addrspace(12)* %v9, {} addrspace(12)* %v10)
+  ret i32 %v11
+; CHECK: ret i32
+}
+
 !0 = !{i64 0, i64 23}
 !1 = !{}
 !2 = distinct !{!2}
